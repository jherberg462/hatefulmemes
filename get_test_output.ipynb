{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#machine learning\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow import keras\n",
    "\n",
    "#accessing files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "#display charts/images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#don't need\n",
    "# from tensorflow.python.keras.preprocessing import sequence\n",
    "# from tensorflow.python.keras.preprocessing import text\n",
    "# import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'image_size': [225, 225],\n",
    "    'text_input': (58,),\n",
    "    'batch_size': 128,\n",
    "    'vocab_size': 30000,\n",
    "    'examples_per_file': 850, #will not change\n",
    "    'test_examples_per_file': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    credentials=None\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "\n",
    "\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file( #file location of GCS private key\n",
    "        '/Users/jeremiahherberg/Downloads/hateful-memes-af65c70c1b79.json')\n",
    "\n",
    "    client = storage.Client(project='hateful-memes', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make into a fn\n",
    "bucket = 'jh_hateful_memes_test'\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)\n",
    "objects = client.list_blobs(bucket, prefix='hatefulmemes_')\n",
    "tfrecords = []\n",
    "for object_ in objects:\n",
    "    path = str(object_).split(', ')[1]\n",
    "    gs_path = os.path.join('gs://', bucket, path)\n",
    "    tfrecords.append(gs_path) #gs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_example_testds(example):\n",
    "    '''\n",
    "    decodes single tfexample from TFrecord file\n",
    "    '''\n",
    "    features = {'id': tf.io.FixedLenFeature([], tf.int64),\n",
    "                'text': tf.io.FixedLenFeature([], tf.string),\n",
    "                'text_lemma': tf.io.FixedLenFeature([], tf.string),\n",
    "                'text_lemma_no_stopwords': tf.io.FixedLenFeature([], tf.string),\n",
    "                'text_no_stopwords':tf.io.FixedLenFeature([], tf.string),\n",
    "                'image': tf.io.FixedLenFeature([], tf.string)}\n",
    "    single_example = tf.io.parse_single_example(example, features)\n",
    "    \n",
    "    text = tf.io.parse_tensor(single_example['text'], out_type=tf.int32)\n",
    "    image = tf.io.decode_jpeg(single_example['image'], 3)\n",
    "    image = tf.image.resize_with_pad(image, *params['image_size'])\n",
    "    image = image / 255.0\n",
    "    label = single_example['id']\n",
    "    return text, image, label\n",
    "\n",
    "def no_tpu_output(text, image, label): #needs to be called last\n",
    "    '''\n",
    "    transforms ds output from text, image, label -> (text, image), label\n",
    "    \n",
    "    args:\n",
    "        text: text output in ds\n",
    "        image: image output in ds\n",
    "        label: label output in ds\n",
    "    returns:\n",
    "        (text, image), label\n",
    "        args will be otherwise unchanged\n",
    "    '''\n",
    "    return (text, image), label\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_ds(files, batch_size=params['batch_size'], tpu=tpu, file_size=params['test_examples_per_file']):\n",
    "    '''\n",
    "    function to create dataset for test data\n",
    "    ***clean up documentation for testds***\n",
    "    args:\n",
    "        files: list of str, filepaths of TFrecord files to be used in DS\n",
    "        batch_size: int, batch size of training/validation step\n",
    "        tpu: bool, default 'tpu' global variable, True is TPU is being used - not a bool update\n",
    "        file_size: int, default num_examples_per_tfrecordfile variable,\n",
    "            number of examples in each TFrecord file\n",
    "    ***todo - update batch_size and file_size to params\n",
    "    returns:\n",
    "        ds: tensorflow input pipeline with images, text and labels\n",
    "            if tpu is not None, output of ds is: text, image, label\n",
    "            if tpu is None, output of ds is: (text, image), label\n",
    "        ds_batches: int, number of steps in each epoch based on the batch_size\n",
    "    '''\n",
    "    ds = tf.data.TFRecordDataset(filenames = files)\n",
    "    ds = ds.map(decode_example_testds, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    #consider adding augmentation to image - can't flip(?)\n",
    "    if tpu is None:\n",
    "        ds = ds.map(no_tpu_output, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=False).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     ds = ds.cache() # -- confirm if dataset is small enough to be cached\n",
    "    \n",
    "    ds_batches = (len(files) * file_size) // batch_size\n",
    "    if (len(files) * file_size) % batch_size > 0:\n",
    "        ds_batches += 1\n",
    "    return ds, ds_batches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds, test_steps = create_test_ds(tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    #get this file to/from a gcs bucket\n",
    "    model = keras.models.load_model('/Users/jeremiahherberg/Downloads/hateful_memes_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_ds.map(lambda img, igs: img), steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ids = next(iter(test_ds.\n",
    "                          map(lambda img, ids:ids).\n",
    "                          unbatch().\n",
    "                          batch(1000))).numpy().astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = {\n",
    "    'id': prediction_ids,\n",
    "    'proba': np.concatenate(predictions),\n",
    "    'label': np.ones(1000, int)\n",
    "}\n",
    "submission_ds = pd.DataFrame(prediction_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ds.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
