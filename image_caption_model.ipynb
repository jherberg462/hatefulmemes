{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJiMk2hxVVdH"
   },
   "outputs": [],
   "source": [
    "#download glove model from http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip and\n",
    "#upload to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iy1dZ-XUJHsF",
    "outputId": "e397ef69-60ff-452a-8f16-a1c9ff035a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "#set random seeds\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#machine learning\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#accessing files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "#display charts/images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#don't need\n",
    "# from tensorflow.python.keras.preprocessing import sequence\n",
    "# from tensorflow.python.keras.preprocessing import text\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ig0i-YXvJHsO"
   },
   "outputs": [],
   "source": [
    "params={\n",
    "    'image_size': [256, 256],\n",
    "    'vocab_size': 10000,\n",
    "    'text_input_length': 49,\n",
    "    'nodes': 256,\n",
    "    'tokenizer_start_index': 58, #index of tokenizer to signal sequence start\n",
    "    'tokenizer_end_index': 57,\n",
    "    'epochs': 20,\n",
    "    'version': 2,\n",
    "    'embedding_dim': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0QY626PJHsU"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    credentials=None\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "\n",
    "\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file( #file location of GCS private key\n",
    "        '/Users/jeremiahherberg/Downloads/hateful-memes-af65c70c1b79.json')\n",
    "\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qa0ralXEJHsX"
   },
   "outputs": [],
   "source": [
    "bucket = 'jh_coco_2014'\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)\n",
    "objects = client.list_blobs(bucket, prefix='coco2014')\n",
    "tfrecords = []\n",
    "for object_ in objects:\n",
    "    path = str(object_).split(', ')[1]\n",
    "    gs_path = os.path.join('gs://', bucket, path)\n",
    "    tfrecords.append(gs_path) #gs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3pADpk0JHsa"
   },
   "outputs": [],
   "source": [
    "# tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6t9Kv3FyJHse",
    "outputId": "cf610950-508d-4fa7-9e6d-7e9d48a95003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arYwtpLjJHsi"
   },
   "outputs": [],
   "source": [
    "def decode_example(example):\n",
    "    '''\n",
    "    decodes single tfexample from TFrecord file\n",
    "    '''\n",
    "    features = {'text': tf.io.FixedLenFeature([], tf.string),\n",
    "                'image': tf.io.FixedLenFeature([], tf.string),\n",
    "                'raw_image': tf.io.FixedLenFeature([], tf.string)}\n",
    "    single_example = tf.io.parse_single_example(example, features)\n",
    "    \n",
    "    text = tf.io.parse_tensor(single_example['text'], out_type=tf.int32)\n",
    "    text = tf.cast(text, tf.float32) \n",
    "    image_features = tf.io.parse_tensor(single_example['image'], out_type=tf.float32)\n",
    "    image = tf.io.decode_jpeg(single_example['raw_image'], 3)\n",
    "    image = tf.image.resize_with_pad(image, *params['image_size'])\n",
    "    image = image / 255.0\n",
    "    # label = tf.cast(label, tf.float32)\n",
    "    return image_features, text\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWKwh9FxouLL"
   },
   "outputs": [],
   "source": [
    "def combine(image, text):\n",
    "    '''\n",
    "    todo - work on documentation\n",
    "    '''\n",
    "    WORDS = tf.math.count_nonzero(text, dtype=tf.int32)\n",
    "    COUNTER = tf.constant(0, dtype=tf.int32)\n",
    "    y  = tf.reshape(text[:,1:WORDS], (-1,1)) #basically free\n",
    "\n",
    "    initial_Xtext = tf.zeros((1, 49))\n",
    "    initial_Xtext = tf.concat([initial_Xtext[:, :COUNTER], \n",
    "                               text[:, COUNTER:COUNTER+1], \n",
    "                               initial_Xtext[:, COUNTER+1:]], axis=-1)\n",
    "    \n",
    "    def condition(counter, img, img2, txt, ini_text, text_out, words ):\n",
    "        return tf.less(counter, words - 2) #2 less than text seq len\n",
    "    \n",
    "    def body(counter, img, img2, txt, ini_text, text_out, words):\n",
    "        \n",
    "#         img = img #clean up img and img2 \n",
    "        img = tf.concat([img, img2], axis=0) #this can be returned unchanged\n",
    "\n",
    "        counter = tf.add(counter, 1) #add +1 to counter\n",
    "\n",
    "\n",
    "        ini_text = tf.concat([ini_text[:, :counter], \n",
    "                              txt[:, counter:counter+1], \n",
    "                              ini_text[:, counter+1:]], axis=-1)\n",
    "        text_out = tf.concat([text_out, ini_text], axis=0)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        return counter, img, img2, txt, ini_text, text_out, words\n",
    "    _, image, _, _, _, txt_out, words= tf.while_loop(condition, \n",
    "                                                     body, \n",
    "                                                     [COUNTER, image, \n",
    "                                                      image, text, \n",
    "                                                      initial_Xtext, initial_Xtext,\n",
    "                                                      WORDS])\n",
    "    return image, txt_out, y, \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVGvXnmBJHsk"
   },
   "outputs": [],
   "source": [
    "def create_ds(files, params):\n",
    "    '''\n",
    "    function to create dataset for training/validation\n",
    "    \n",
    "    args:\n",
    "        files: list of str, filepaths of TFrecord files to be used in DS\n",
    "        params: dict with the following keys:\n",
    "            batch_size: int, batch size of training/validation step\n",
    "            examples_per_file: int, number of examples in each TFrecord file\n",
    "        train, bool, default True, indicator if the DS is for training\n",
    "        test_examples, int: default 1000 number of examples in test dataset\n",
    "    returns:\n",
    "        ds: tensorflow input pipeline with images, text and labels\n",
    "            output of ds is: (text, image), label\n",
    "        ds_batches: int, number of steps in each epoch based on the batch_size\n",
    "    '''\n",
    "    batch_size = 1\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(filenames = files)\n",
    "    ds = ds.map(decode_example, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # ds = ds.map(combine)\n",
    "\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "#     ds = ds.cache() \n",
    "    \n",
    "    return ds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKHCPu3cZf57"
   },
   "outputs": [],
   "source": [
    "def download_file(client, bucket, file_name):\n",
    "    '''\n",
    "    downloads a file from a GCS bucket into working directory\n",
    "\n",
    "    args:\n",
    "        client: google.cloud.storage.Client object\n",
    "        bucket: str, name of bucket to download file from\n",
    "        file_name: str, file name to download\n",
    "    returns: None\n",
    "    \n",
    "    '''\n",
    "    _bucket = client.bucket(bucket)\n",
    "    blob = _bucket.blob(file_name)\n",
    "    blob.download_to_filename(file_name)\n",
    "\n",
    "def create_tokenizer_from_filename(file_name,\n",
    "                                  client=None,\n",
    "                                  bucket=None):\n",
    "    '''\n",
    "    creates tf.keras.preprocessing.text.tokenizer from a \n",
    "    json config file in current working directory\n",
    "    args:\n",
    "        file_name: str, filename where config json file is located\n",
    "        client, google.cloud.storage.Client object, default None, if an arg\n",
    "            is passed, function will first check if glove_file exists in current\n",
    "            directory, and if not, will download an object located at glove_file\n",
    "            in the bucket passed into bucket arg\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file\n",
    "    returns:\n",
    "        tokenizer object\n",
    "    '''\n",
    "    if client:\n",
    "        if not os.path.isfile(file_name):\n",
    "            download_file(client, bucket,file_name)\n",
    "    with open(file_name) as file:\n",
    "        open_file = json.load(file)\n",
    "        tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(open_file)\n",
    "    return tokenizer\n",
    "\n",
    "def get_embedding_weights_from_tokenizer_glove(glove_file,\n",
    "                                              tokenizer,\n",
    "                                              embedding_dim,\n",
    "                                              client=None,\n",
    "                                              bucket=None,\n",
    "                                              ):\n",
    "    '''\n",
    "    gets the weights to use in an embedding layer from a pretained\n",
    "    model based on the tokenizer used to create sequences that will\n",
    "    be passed into embedding layer\n",
    "    \n",
    "    args:\n",
    "        glove_file: str, path of pretrained model from current directory\n",
    "        tokenizer: tf.keras.preprocessing.text.tokenizer object, tokenizer\n",
    "            that was used to create sequences\n",
    "        embedding_dim: int, output_dim of embedding layer of pre-trained model\n",
    "        client, google.cloud.storage.Client object, default None, if an arg\n",
    "            is passed, function will first check if glove_file exists in current\n",
    "            directory, and if not, will download an object located at glove_file\n",
    "            in the bucket passed into bucket arg\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file\n",
    "    returns: \n",
    "        embedding_weights: numpy array, shaped* (vocab_size, embedding_dim)\n",
    "            weights that can be used for embedding layer\n",
    "            *vocab_size = tokenizer.num_words which is the number of words in\n",
    "            the tokenizer vocabulary\n",
    "        \n",
    "    '''\n",
    "    if client:\n",
    "        if not os.path.isfile(glove_file):\n",
    "            download_file(client, bucket, glove_file)\n",
    "    word_values = dict()\n",
    "    file = open(glove_file, encoding='utf-8')\n",
    "    \n",
    "    for line in file:\n",
    "        coeff = line.split()\n",
    "        word = coeff[0]\n",
    "        coefficients = np.asarray(coeff[-300:], dtype='float32')\n",
    "        word_values[word] = coefficients\n",
    "    file.close()\n",
    "    vocab_size = tokenizer.num_words\n",
    "    embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx < vocab_size:\n",
    "            word_embedding_values = word_values.get(word)\n",
    "            if word_embedding_values is not None:\n",
    "                embedding_weights[idx] = word_embedding_values\n",
    "    \n",
    "    return embedding_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kx474nk2JHsm"
   },
   "outputs": [],
   "source": [
    "ds = create_ds(tfrecords, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKb5yx6PJHsr"
   },
   "outputs": [],
   "source": [
    "def create_model(params, embedding_weights=None):\n",
    "    '''\n",
    "    creates model to caption images\n",
    "    '''\n",
    "    vocab_size = params['vocab_size']\n",
    "    txt_input_length = params['text_input_length']\n",
    "    nodes = params['nodes']\n",
    "    embedding_dim = params['embedding_dim']\n",
    "\n",
    "    image_feature_inp = layers.Input((64, 2048), name='features_input')\n",
    "    features = layers.Reshape((8, 8, 2048))(image_feature_inp)\n",
    "    features = layers.GlobalAveragePooling2D()(features)\n",
    "    # features = layers.Flatten()(image_feature_inp)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    features = layers.Dense(nodes)(features)\n",
    "    features = layers.LeakyReLU()(features)\n",
    "    \n",
    "    txt_inp = layers.Input((txt_input_length,), name='text_input')\n",
    "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(txt_inp)\n",
    "    embedding = layers.Dropout(0.5)(embedding)\n",
    "    sequences = layers.LSTM(nodes)(embedding)\n",
    "\n",
    "    features_tst = features #layers.LSTM(nodes)(features) #not sure if this is appropiate\n",
    "    #if above doesn't work well, flatten after image_feature_imp\n",
    "    decoder = layers.Add()([features_tst, sequences]) # Concatenate - try\n",
    "    decoder = layers.Dense(nodes, activation=None)(decoder)\n",
    "    decoder = layers.LeakyReLU()(decoder)\n",
    "    output = layers.Dense(vocab_size, activation='softmax')(decoder)\n",
    "    model = keras.Model([image_feature_inp, txt_inp], output)\n",
    "    model.layers[5].set_weights([embedding_weights])\n",
    "    model.layers[5].trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8DlDjMDZf6A"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer_from_filename('coco_tokenizer.json', \n",
    "                                           client,\n",
    "                                           'jh_coco_2014')\n",
    "embedding_weights = get_embedding_weights_from_tokenizer_glove('glove.840B.300d.txt',\n",
    "                                                               tokenizer,\n",
    "                                                               300,\n",
    "                                                               client,\n",
    "                                                               'jh_hateful_memes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVGtPCLSJHst"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = create_model(params, embedding_weights) #embedding_weights\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_tracker = tf.keras.metrics.Mean(name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KnC_gdymcvT7"
   },
   "outputs": [],
   "source": [
    "# model.layers[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYB3RgKvJHsv"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    '''\n",
    "    taken from https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "    '''\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    loss = tf.reduce_mean(loss_)\n",
    "    #update loss tracker\n",
    "    loss_tracker.update_state(loss)\n",
    "\n",
    "    return loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIaBARmZJHsx"
   },
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_step(image_, text_, y): # params\n",
    "    '''\n",
    "    todo - work on documentation\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    def step(image, text, y_value):\n",
    "        \n",
    "        img_tmp = image\n",
    "#         for _ in range(y_len_): \n",
    "#             image = tf.concat([image, img_tmp], axis=0)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model((image, text), training=True)\n",
    "            loss = loss_function(y_value, preds)\n",
    "            trainable_variables = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        # return preds, loss\n",
    "    #preds, loss = \n",
    "    strategy.run(step, args=(image_, text_, y))\n",
    "    # return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99PbKbzsNDoy"
   },
   "outputs": [],
   "source": [
    "epochs = params['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3FjLQMcrZf6I",
    "outputId": "badc1028-2acd-4d83-dabc-ea873f37993c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step:10000, loss: 4.19978, batch time:350.110\n",
      "epoch: 1, step:20000, loss: 4.00336, batch time:783.773\n",
      "epoch: 1, step:30000, loss: 3.92560, batch time:1213.035\n",
      "epoch: 1, step:40000, loss: 3.87366, batch time:1618.746\n",
      "epoch: 1, step:50000, loss: 3.85100, batch time:2029.504\n",
      "epoch: 1, step:60000, loss: 3.81980, batch time:2594.487\n",
      "epoch:1, loss:3.81273, time:3046.147, steps:69962\n",
      "epoch: 2, step:10000, loss: 3.91460, batch time:304.784\n",
      "epoch: 2, step:20000, loss: 3.83234, batch time:650.981\n",
      "epoch: 2, step:30000, loss: 3.80674, batch time:1010.529\n",
      "epoch: 2, step:40000, loss: 3.78397, batch time:1405.861\n",
      "epoch: 2, step:50000, loss: 3.77634, batch time:1804.138\n",
      "epoch: 2, step:60000, loss: 3.75555, batch time:2202.947\n",
      "epoch:2, loss:3.75696, time:2516.154, steps:69962\n",
      "epoch: 3, step:10000, loss: 3.93835, batch time:288.590\n",
      "epoch: 3, step:20000, loss: 3.85474, batch time:618.488\n",
      "epoch: 3, step:30000, loss: 3.82820, batch time:941.615\n",
      "epoch: 3, step:40000, loss: 3.80019, batch time:1254.976\n",
      "epoch: 3, step:50000, loss: 3.79178, batch time:1539.790\n",
      "epoch: 3, step:60000, loss: 3.76825, batch time:1826.272\n",
      "epoch:3, loss:3.76848, time:2150.429, steps:69962\n",
      "epoch: 4, step:10000, loss: 3.94814, batch time:287.788\n",
      "epoch: 4, step:20000, loss: 3.85955, batch time:614.070\n",
      "epoch: 4, step:30000, loss: 3.82958, batch time:898.806\n",
      "epoch: 4, step:40000, loss: 3.80175, batch time:1185.413\n",
      "epoch: 4, step:50000, loss: 3.79127, batch time:1444.523\n",
      "epoch: 4, step:60000, loss: 3.76754, batch time:1708.164\n",
      "epoch:4, loss:3.76704, time:1970.760, steps:69962\n",
      "epoch: 5, step:10000, loss: 3.94540, batch time:266.534\n",
      "epoch: 5, step:20000, loss: 3.85994, batch time:556.806\n",
      "epoch: 5, step:30000, loss: 3.83194, batch time:830.360\n",
      "epoch: 5, step:40000, loss: 3.80268, batch time:1112.784\n",
      "epoch: 5, step:50000, loss: 3.79344, batch time:1374.781\n",
      "epoch: 5, step:60000, loss: 3.76684, batch time:1675.832\n",
      "epoch:5, loss:3.76648, time:1977.620, steps:69962\n",
      "epoch: 6, step:10000, loss: 3.93918, batch time:326.216\n",
      "epoch: 6, step:20000, loss: 3.84993, batch time:671.862\n",
      "epoch: 6, step:30000, loss: 3.82553, batch time:979.732\n",
      "epoch: 6, step:40000, loss: 3.79806, batch time:1305.860\n",
      "epoch: 6, step:50000, loss: 3.78678, batch time:1617.227\n",
      "epoch: 6, step:60000, loss: 3.76091, batch time:1896.454\n",
      "epoch:6, loss:3.76069, time:2204.502, steps:69962\n",
      "epoch: 7, step:10000, loss: 3.93156, batch time:328.539\n",
      "epoch: 7, step:20000, loss: 3.84438, batch time:641.086\n",
      "epoch: 7, step:30000, loss: 3.81949, batch time:975.121\n",
      "epoch: 7, step:40000, loss: 3.79318, batch time:1298.496\n",
      "epoch: 7, step:50000, loss: 3.78415, batch time:1562.112\n",
      "epoch: 7, step:60000, loss: 3.75885, batch time:1865.179\n",
      "epoch:7, loss:3.75859, time:2168.465, steps:69962\n",
      "epoch: 8, step:10000, loss: 3.93468, batch time:294.096\n",
      "epoch: 8, step:20000, loss: 3.84778, batch time:601.838\n",
      "epoch: 8, step:30000, loss: 3.82299, batch time:899.629\n",
      "epoch: 8, step:40000, loss: 3.79312, batch time:1219.057\n",
      "epoch: 8, step:50000, loss: 3.78215, batch time:1512.755\n",
      "epoch: 8, step:60000, loss: 3.75677, batch time:1809.267\n",
      "epoch:8, loss:3.75587, time:2075.654, steps:69962\n",
      "epoch: 9, step:10000, loss: 3.93109, batch time:315.727\n",
      "epoch: 9, step:20000, loss: 3.84445, batch time:724.010\n",
      "epoch: 9, step:30000, loss: 3.81920, batch time:1043.513\n",
      "epoch: 9, step:40000, loss: 3.79087, batch time:1343.948\n",
      "epoch: 9, step:50000, loss: 3.78096, batch time:1616.379\n",
      "epoch: 9, step:60000, loss: 3.75392, batch time:1885.312\n",
      "epoch:9, loss:3.75387, time:2149.751, steps:69962\n",
      "epoch: 10, step:10000, loss: 3.92245, batch time:291.975\n",
      "epoch: 10, step:20000, loss: 3.83683, batch time:598.693\n",
      "epoch: 10, step:30000, loss: 3.81251, batch time:881.356\n",
      "epoch: 10, step:40000, loss: 3.78628, batch time:1171.154\n",
      "epoch: 10, step:50000, loss: 3.77613, batch time:1432.435\n",
      "epoch: 10, step:60000, loss: 3.75013, batch time:1723.476\n",
      "epoch:10, loss:3.74851, time:2034.579, steps:69962\n",
      "epoch: 11, step:10000, loss: 3.91725, batch time:335.352\n",
      "epoch: 11, step:20000, loss: 3.83813, batch time:690.514\n",
      "epoch: 11, step:30000, loss: 3.81384, batch time:1019.612\n",
      "epoch: 11, step:40000, loss: 3.78511, batch time:1323.743\n",
      "epoch: 11, step:50000, loss: 3.77511, batch time:1618.009\n",
      "epoch: 11, step:60000, loss: 3.75032, batch time:1940.293\n",
      "epoch:11, loss:3.75011, time:2226.909, steps:69962\n",
      "epoch: 12, step:10000, loss: 3.92907, batch time:339.966\n",
      "epoch: 12, step:20000, loss: 3.84018, batch time:680.948\n",
      "epoch: 12, step:30000, loss: 3.81578, batch time:1034.493\n",
      "epoch: 12, step:40000, loss: 3.78670, batch time:1382.167\n",
      "epoch: 12, step:50000, loss: 3.77650, batch time:1672.118\n",
      "epoch: 12, step:60000, loss: 3.74996, batch time:1969.172\n",
      "epoch:12, loss:3.74967, time:2297.796, steps:69962\n",
      "epoch: 13, step:10000, loss: 3.91706, batch time:388.427\n",
      "epoch: 13, step:20000, loss: 3.83388, batch time:776.618\n",
      "epoch: 13, step:30000, loss: 3.80817, batch time:1154.278\n",
      "epoch: 13, step:40000, loss: 3.78199, batch time:1465.549\n",
      "epoch: 13, step:50000, loss: 3.77177, batch time:1757.768\n",
      "epoch: 13, step:60000, loss: 3.74735, batch time:2053.352\n",
      "epoch:13, loss:3.74742, time:2371.651, steps:69962\n",
      "epoch: 14, step:10000, loss: 3.92346, batch time:414.245\n",
      "epoch: 14, step:20000, loss: 3.83844, batch time:781.818\n",
      "epoch: 14, step:30000, loss: 3.81267, batch time:1110.296\n",
      "epoch: 14, step:40000, loss: 3.78472, batch time:1448.557\n",
      "epoch: 14, step:50000, loss: 3.77204, batch time:1778.357\n",
      "epoch: 14, step:60000, loss: 3.74680, batch time:2088.902\n",
      "epoch:14, loss:3.74671, time:2390.044, steps:69962\n",
      "epoch: 15, step:10000, loss: 3.92290, batch time:334.277\n",
      "epoch: 15, step:20000, loss: 3.83302, batch time:644.361\n",
      "epoch: 15, step:30000, loss: 3.81147, batch time:932.018\n",
      "epoch: 15, step:40000, loss: 3.78386, batch time:1265.768\n",
      "epoch: 15, step:50000, loss: 3.77273, batch time:1596.151\n",
      "epoch: 15, step:60000, loss: 3.74810, batch time:1873.425\n",
      "epoch:15, loss:3.74825, time:2139.663, steps:69962\n",
      "epoch: 16, step:10000, loss: 3.91515, batch time:315.709\n",
      "epoch: 16, step:20000, loss: 3.83161, batch time:629.693\n",
      "epoch: 16, step:30000, loss: 3.80869, batch time:951.865\n",
      "epoch: 16, step:40000, loss: 3.78195, batch time:1242.117\n",
      "epoch: 16, step:50000, loss: 3.77000, batch time:1547.577\n",
      "epoch: 16, step:60000, loss: 3.74534, batch time:1816.267\n",
      "epoch:16, loss:3.74542, time:2129.230, steps:69962\n",
      "epoch: 17, step:10000, loss: 3.92417, batch time:411.544\n",
      "epoch: 17, step:20000, loss: 3.83642, batch time:841.045\n",
      "epoch: 17, step:30000, loss: 3.81215, batch time:1209.224\n",
      "epoch: 17, step:40000, loss: 3.78289, batch time:1538.424\n",
      "epoch: 17, step:50000, loss: 3.77246, batch time:1853.997\n",
      "epoch: 17, step:60000, loss: 3.74604, batch time:2150.323\n",
      "epoch:17, loss:3.74499, time:2445.980, steps:69962\n",
      "epoch: 18, step:10000, loss: 3.91614, batch time:432.551\n",
      "epoch: 18, step:20000, loss: 3.83244, batch time:852.808\n",
      "epoch: 18, step:30000, loss: 3.81093, batch time:1243.204\n",
      "epoch: 18, step:40000, loss: 3.78387, batch time:1562.195\n",
      "epoch: 18, step:50000, loss: 3.77260, batch time:1926.744\n",
      "epoch: 18, step:60000, loss: 3.74698, batch time:2273.031\n",
      "epoch:18, loss:3.74782, time:2595.294, steps:69962\n",
      "epoch: 19, step:10000, loss: 3.92653, batch time:444.913\n",
      "epoch: 19, step:20000, loss: 3.84073, batch time:875.465\n",
      "epoch: 19, step:30000, loss: 3.81670, batch time:1324.123\n",
      "epoch: 19, step:40000, loss: 3.78788, batch time:1800.721\n",
      "epoch: 19, step:50000, loss: 3.77635, batch time:2137.257\n",
      "epoch: 19, step:60000, loss: 3.75032, batch time:2468.893\n",
      "epoch:19, loss:3.74974, time:2796.684, steps:69962\n",
      "epoch: 20, step:10000, loss: 3.92734, batch time:488.069\n",
      "epoch: 20, step:20000, loss: 3.83939, batch time:928.105\n",
      "epoch: 20, step:30000, loss: 3.81181, batch time:1389.033\n",
      "epoch: 20, step:40000, loss: 3.78438, batch time:1862.233\n",
      "epoch: 20, step:50000, loss: 3.77117, batch time:2304.845\n",
      "epoch: 20, step:60000, loss: 3.74384, batch time:2681.664\n",
      "epoch:20, loss:3.74363, time:3097.855, steps:69962\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    step = 0\n",
    "    for image, text in ds:\n",
    "        img, txt, y = combine(image, text)\n",
    "        _ = train_step(img, txt, y)\n",
    "        step +=1\n",
    "\n",
    "\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            batch_time = time.time()\n",
    "            time_batch = batch_time - epoch_start\n",
    "            print('epoch: {}, step:{}, loss: {:.5f}, batch time:{:.3f}'.format(epoch +1,\n",
    "                                                                      step,\n",
    "                                                                      loss_tracker.result().numpy(),\n",
    "                                                                      time_batch))\n",
    "\n",
    "    batch_time = time.time()\n",
    "    time_batch = batch_time - epoch_start\n",
    "    print('epoch:{}, loss:{:.5f}, time:{:.3f}, steps:{}'.format(epoch+1, loss_tracker.result().numpy(), time_batch, step))\n",
    "    loss_tracker.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5I0r55eZf6J"
   },
   "outputs": [],
   "source": [
    "model_num = params['version']\n",
    "model_path = 'image_caption_model_v{}.h5'.format(model_num)\n",
    "model.save(model_path)\n",
    "model_bucket = client.bucket('jh_hateful_memes')\n",
    "blob = model_bucket.blob(model_path)\n",
    "blob.upload_from_filename(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ea2kjWldUB6v"
   },
   "outputs": [],
   "source": [
    "def inference(params, image_feature, image, model, tokenizer):\n",
    "    '''\n",
    "    uses an image captioning model to generate a caption of an image\n",
    "    \n",
    "    args:\n",
    "        params: dictionary with at least the following keys:\n",
    "            tokenizer_start_index: int, tokenizer value that signals start\n",
    "            of caption\n",
    "            tokenizer_end_index: int, tokenizer value that signals end of\n",
    "            caption\n",
    "            text_input_length: int, len of text input of model\n",
    "        image_feature: array, shaped (1, 64, 2048) output of an image being\n",
    "            passed through InceptionV3 model without classification layer\n",
    "        model: tensorflow functional model, model to generate caption\n",
    "        tokenizer: tf.keras.preprocessing.text.tokenizer object, \n",
    "    '''\n",
    "    text_len = params['text_input_length']\n",
    "    text = np.zeros((1, text_len))\n",
    "    results = list()\n",
    "    result = params['tokenizer_start_index']\n",
    "    for idx in range(text_len):\n",
    "        text[:, idx] = result\n",
    "        result = model((image_features, text))\n",
    "        result = tf.argmax(result[0]).numpy()\n",
    "        if result == params['tokenizer_end_index']:\n",
    "            break\n",
    "        results.append(result)\n",
    "    results_converted = tokenizer.sequences_to_texts([results])[0]\n",
    "    print(results_converted)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTilBHCpnKWR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "image_caption_model (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
