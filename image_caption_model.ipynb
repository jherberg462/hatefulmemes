{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qJiMk2hxVVdH"
   },
   "outputs": [],
   "source": [
    "#download glove model from http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip and\n",
    "#upload to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Iy1dZ-XUJHsF",
    "outputId": "828b58f5-a225-4484-8f0a-b32faa78cf02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "#set random seeds\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#machine learning\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#accessing files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "#display charts/images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#don't need\n",
    "# from tensorflow.python.keras.preprocessing import sequence\n",
    "# from tensorflow.python.keras.preprocessing import text\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ig0i-YXvJHsO"
   },
   "outputs": [],
   "source": [
    "params={\n",
    "    'image_size': [256, 256],\n",
    "    'vocab_size': 10000,\n",
    "    'text_input_length': 49,\n",
    "    'nodes': 256,\n",
    "    'tokenizer_start_index': 58, #index of tokenizer to signal sequence start\n",
    "    'tokenizer_end_index': 57,\n",
    "    'epochs': 20,\n",
    "    'version': 8,\n",
    "    'embedding_dim': 300,\n",
    "    'ds_size': 801592,\n",
    "    'batch_size': 5343\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-0QY626PJHsU"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "k3pADpk0JHsa"
   },
   "outputs": [],
   "source": [
    "training_bucket = 'gs://kds-e7996502fe373b391a0a14641ad5f932ee7d607744dbe970cc8ffe08'\n",
    "glove_bucket = 'gs://kds-5123f8991f380aa8ec3a0dfae64a3732b529d4e504450dd8f9e55fb1'\n",
    "tfrecords = tf.io.gfile.glob(training_bucket + '/*tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XOOAMRQe7Nob",
    "outputId": "1240780b-83ac-4f14-f9b7-fdc62ad489de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://kds-5123f8991f380aa8ec3a0dfae64a3732b529d4e504450dd8f9e55fb1/glove.840B.300d.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.io.gfile.glob(glove_bucket + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "id": "6t9Kv3FyJHse",
    "outputId": "9c044c84-01f9-4818-bbc7-a1da8017aadd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.55.49.66:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.55.49.66:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.55.49.66:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "arYwtpLjJHsi"
   },
   "outputs": [],
   "source": [
    "def decode_example(example):\n",
    "    '''\n",
    "    decodes single tfexample from TFrecord file\n",
    "    '''\n",
    "    features = {'text': tf.io.FixedLenFeature([], tf.string),\n",
    "                'inception': tf.io.FixedLenFeature([], tf.string), #can also be vgg\n",
    "                'y': tf.io.FixedLenFeature([], tf.string)}\n",
    "    single_example = tf.io.parse_single_example(example, features)\n",
    "    \n",
    "    text = tf.io.parse_tensor(single_example['text'], out_type=tf.int32)\n",
    "    text = tf.cast(text, tf.float32)\n",
    "    image_features = tf.io.parse_tensor(single_example['inception'], out_type=tf.float32)\n",
    "    y_value = tf.io.parse_tensor(single_example['y'], out_type=tf.int32)\n",
    "    y_value = tf.expand_dims(y_value, axis=0)\n",
    "\n",
    "    return (image_features, text), y_value\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HVGvXnmBJHsk"
   },
   "outputs": [],
   "source": [
    "def create_ds(files, params):\n",
    "    '''\n",
    "    function to create dataset for training/validation\n",
    "    \n",
    "    args:\n",
    "        files: list of str, filepaths of TFrecord files to be used in DS\n",
    "        params: dict with the following keys:\n",
    "            batch_size: int, batch size of training/validation step\n",
    "            examples_per_file: int, number of examples in each TFrecord file\n",
    "        train, bool, default True, indicator if the DS is for training\n",
    "        test_examples, int: default 1000 number of examples in test dataset\n",
    "    returns:\n",
    "        ds: tensorflow input pipeline with images, text and labels\n",
    "            output of ds is: (text, image), label\n",
    "        ds_batches: int, number of steps in each epoch based on the batch_size\n",
    "    '''\n",
    "    # batch_size = params['batch_size']\n",
    "    batch_size = params['ds_size']\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(filenames = files)\n",
    "    ds = ds.map(decode_example, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "#     ds = ds.cache() \n",
    "    \n",
    "    return ds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NKHCPu3cZf57"
   },
   "outputs": [],
   "source": [
    "def download_file(bucket, file_name):\n",
    "    '''\n",
    "    downloads a file from a public GCS bucket into working directory\n",
    "\n",
    "    args:\n",
    "        bucket: str, name of bucket to download file from\n",
    "        file_name: str, file name to download\n",
    "    returns: None\n",
    "    \n",
    "    '''\n",
    "    file_path = tf.io.gfile.glob(bucket + '/' + file_name)[0]\n",
    "    tf.io.gfile.copy(file_path, file_name)\n",
    "\n",
    "def create_tokenizer_from_filename(file_name,\n",
    "                                  bucket=None):\n",
    "    '''\n",
    "    creates tf.keras.preprocessing.text.tokenizer from a \n",
    "    json config file in current working directory\n",
    "    args:\n",
    "        file_name: str, filename where config json file is located\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file, if an arg\n",
    "            is passed, function will first check if file_name exists in current\n",
    "            directory, and if not, will download an object located at file_name\n",
    "            in the bucket passed into bucket arg\n",
    "    returns:\n",
    "        tokenizer object\n",
    "    '''\n",
    "    if bucket:\n",
    "        if not os.path.isfile(file_name):\n",
    "            download_file(bucket,file_name)\n",
    "    with open(file_name) as file:\n",
    "        open_file = json.load(file)\n",
    "        tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(open_file)\n",
    "    return tokenizer\n",
    "\n",
    "def get_embedding_weights_from_tokenizer_glove(glove_file,\n",
    "                                              tokenizer,\n",
    "                                              embedding_dim,\n",
    "                                              bucket=None,\n",
    "                                              ):\n",
    "    '''\n",
    "    gets the weights to use in an embedding layer from a pretained\n",
    "    model based on the tokenizer used to create sequences that will\n",
    "    be passed into embedding layer\n",
    "    \n",
    "    args:\n",
    "        glove_file: str, path of pretrained model from current directory\n",
    "        tokenizer: tf.keras.preprocessing.text.tokenizer object, tokenizer\n",
    "            that was used to create sequences\n",
    "        embedding_dim: int, output_dim of embedding layer of pre-trained model\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file, if an arg\n",
    "            is passed, function will first check if glove_file exists in current\n",
    "            directory, and if not, will download an object located at glove_file\n",
    "            in the bucket passed into bucket arg\n",
    "    returns: \n",
    "        embedding_weights: numpy array, shaped* (vocab_size, embedding_dim)\n",
    "            weights that can be used for embedding layer\n",
    "            *vocab_size = tokenizer.num_words which is the number of words in\n",
    "            the tokenizer vocabulary\n",
    "        \n",
    "    '''\n",
    "    if bucket:\n",
    "        if not os.path.isfile(glove_file):\n",
    "            download_file(bucket, glove_file)\n",
    "    word_values = dict()\n",
    "    file = open(glove_file, encoding='utf-8')\n",
    "    \n",
    "    for line in file:\n",
    "        coeff = line.split()\n",
    "        word = coeff[0]\n",
    "        coefficients = np.asarray(coeff[-embedding_dim:], dtype='float32')\n",
    "        word_values[word] = coefficients\n",
    "    file.close()\n",
    "    vocab_size = tokenizer.num_words\n",
    "    embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx < vocab_size:\n",
    "            word_embedding_values = word_values.get(word)\n",
    "            if word_embedding_values is not None:\n",
    "                embedding_weights[idx] = word_embedding_values\n",
    "    \n",
    "    return embedding_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kx474nk2JHsm"
   },
   "outputs": [],
   "source": [
    "ds = create_ds(tfrecords, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NKb5yx6PJHsr"
   },
   "outputs": [],
   "source": [
    "def create_model(params, embedding_weights=None):\n",
    "    '''\n",
    "    creates model to caption images\n",
    "    '''\n",
    "    vocab_size = params['vocab_size']\n",
    "    txt_input_length = params['text_input_length']\n",
    "    nodes = params['nodes']\n",
    "    embedding_dim = params['embedding_dim']\n",
    "\n",
    "    image_feature_inp = layers.Input((2048,), name='features_input')\n",
    "    \n",
    "    \n",
    "    txt_inp = layers.Input((txt_input_length,), name='text_input')\n",
    "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(txt_inp)\n",
    "\n",
    "    \n",
    "    encoder = layers.Dense(embedding_dim, activation='relu')(embedding)\n",
    "    \n",
    "    w1 = layers.Dense(nodes)(image_feature_inp)\n",
    "    w2 = layers.Dense(nodes)(encoder)\n",
    "    attention = layers.Add()([w1, w2])\n",
    "    attention = layers.Dense(1)(attention)\n",
    "    attention = tf.nn.tanh(attention)\n",
    "    encoder_model = keras.Model([txt_inp, image_feature_inp], [attention, encoder])\n",
    "    \n",
    "    image_context_input = layers.Input((embedding_dim,), name='context_vector')\n",
    "    image_context = tf.expand_dims(image_context_input, 1)\n",
    "    \n",
    "    x = tf.concat([image_context, embedding], axis=1)\n",
    "    x = layers.Bidirectional(layers.GRU(units=nodes, return_sequences=False,))(x)\n",
    "    # x = layers.GRU(units=nodes, return_sequences=False,)(x)\n",
    "    x = layers.Dense(nodes)(x)\n",
    "    decoder_output = layers.Dense(vocab_size)(x)\n",
    "    decoder_model = keras.Model([image_context_input, txt_inp], [decoder_output])\n",
    "    if embedding_weights is not None:\n",
    "        decoder_model.layers[3].set_weights([embedding_weights])\n",
    "        decoder_model.layers[3].trainable = False\n",
    "        \n",
    "        encoder_model.layers[1].set_weights([embedding_weights])\n",
    "        encoder_model.layers[1].trainable = False\n",
    "\n",
    "    image_input = layers.Input(2048, name='image_input')\n",
    "    text_input = layers.Input(params['text_input_length'], name='text_input')\n",
    "    attention_output, encoder_output = encoder_model((text_input, image_input))\n",
    "    attention_weights = tf.nn.softmax(attention_output, axis=1)\n",
    "    # context_vector = attention_weights * encoder_output\n",
    "    context_vector = tf.multiply(x=attention_weights, y=encoder_output)\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    preds = decoder_model((context_vector, text_input))\n",
    "    final_model = keras.Model([image_input, text_input], preds)\n",
    "    # final_model.compile(optimizer='adam',loss='SparseCategoricalCrossentropy')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-8DlDjMDZf6A"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer_from_filename('coco_tokenizer.json', \n",
    "                                           training_bucket)\n",
    "embedding_weights = get_embedding_weights_from_tokenizer_glove('glove.840B.300d.txt',\n",
    "                                                               tokenizer,\n",
    "                                                               300,\n",
    "                                                               glove_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JkmvmncofYQM"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    '''\n",
    "    taken from https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "    '''\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    loss = tf.reduce_mean(loss_)\n",
    "    #update loss tracker\n",
    "    # loss_tracker.update_state(loss)\n",
    "\n",
    "    return loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-3DhhxMuIsru"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # encoder, decoder = create_model(params, embedding_weights) #embedding_weights\n",
    "    model = create_model(params, embedding_weights)\n",
    "    optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "    model.compile(optimizer='adam', loss=loss_function)\n",
    "    # loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "    # model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "bVGtPCLSJHst",
    "outputId": "cb4e5b81-2f59-418d-84b7-09d2ea320fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                patience=15,\n",
    "                                mode='min',\n",
    "                                restore_best_weights=True)\n",
    "for x, y in ds:\n",
    "    history = model.fit(x, y, steps_per_epoch=150, epochs=5000, verbose=0,\n",
    "                        callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "s2Nm6UP7e2jz",
    "outputId": "66865c1c-97fb-4e1a-998d-ef597da5a8b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    credentials=None\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "\n",
    "\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file( #file location of GCS private key\n",
    "        '/Users/jeremiahherberg/Downloads/hateful-memes-af65c70c1b79.json')\n",
    "\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "Z5I0r55eZf6J",
    "outputId": "c0cc9b29-57a9-4b2b-f101-79034b8c6566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "model_num = params['version']\n",
    "model_path = 'image_caption_model_v{}.h5'.format(model_num)\n",
    "model.save(model_path)\n",
    "model_bucket = client.bucket('jh_hateful_memes')\n",
    "blob = model_bucket.blob(model_path)\n",
    "blob.upload_from_filename(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ll0ab_UfVlvi"
   },
   "outputs": [],
   "source": [
    "def get_image_captions(params, images):\n",
    "    '''\n",
    "    creates captions to a group of images\n",
    "    \n",
    "    args:\n",
    "        params: dictionary with at least the following keys:\n",
    "            caption_text_input_length: int, length of captions\n",
    "            tokenizer_start_index: int, value to signal start of caption\n",
    "            tokenizer_end_index: int, value to signal end of caption\n",
    "            \n",
    "        images: tensor, dtype: tf.float32 shaped (None, 299, 299, 3) None is the \n",
    "        number of images, each image should be normalized to have\n",
    "        pixel values of -1 to 1. Images to be captioned\n",
    "\n",
    "            \n",
    "    returns:\n",
    "        captions: tensor, dtype float, shaped \n",
    "        (None, params['caption_text_input_length'])None is the number of \n",
    "        images, image caption sequences\n",
    "    '''\n",
    "    num_images = len(images)\n",
    "    caption_len = params['text_input_length']\n",
    "    caption_end_index = params['tokenizer_end_index']\n",
    "\n",
    "    @tf.function\n",
    "    def get_capt(img, txt):\n",
    "        def caption_step(image_, text_):\n",
    "            '''\n",
    "            evaluate model here\n",
    "            '''\n",
    "            txt_ = tf.expand_dims(text_, axis=0)\n",
    "            pred = model((image_, txt_))\n",
    "\n",
    "\n",
    "            return pred\n",
    "        result = strategy.run(caption_step, args=(img, txt))\n",
    "        return result\n",
    "\n",
    "    \n",
    "    captions = list()\n",
    "    for image in range(num_images):\n",
    "        img = images[image]\n",
    "        img = tf.expand_dims(img, axis=0)\n",
    "        txt_input = np.zeros((caption_len))\n",
    "        result = params['tokenizer_start_index']\n",
    "        for idx in range(caption_len):\n",
    "            txt_input[idx] = result\n",
    "            # with tf.device('/TPU:0'):\n",
    "            #     result = caption_step(img, txt_input)\n",
    "                # result = strategy.run(caption_step, args=(img, txt_input))\n",
    "            result = get_capt(img, txt_input)\n",
    "            result = result.numpy()[0] # result.values[0].numpy()[0]\n",
    "            result = tf.argmax(result, axis=0)\n",
    "            if result == caption_end_index:\n",
    "                break\n",
    "        captions.append(txt_input)\n",
    "    captions = tf.convert_to_tensor(captions)\n",
    "    return captions\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IDjhW_VF9HfO"
   },
   "outputs": [],
   "source": [
    "def plot_metric(metric1, ylabel):\n",
    "    plt.plot(history.history[metric1], label=metric1)\n",
    "    # plt.plot(history.history[metric2], label=metric2)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "fVb6NnhPF6FS",
    "outputId": "3d89d31f-08af-4f60-9f1c-0c435c3b726b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric('loss', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jZc8L4MGe2j8",
    "outputId": "8301bc04-0c54-44df-8518-18a30d89da56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2030678987503052"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6oYiyR_MIsr8"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "image_caption_model_draft-4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
