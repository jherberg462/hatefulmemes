{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJiMk2hxVVdH"
   },
   "outputs": [],
   "source": [
    "#download glove model from http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip and\n",
    "#upload to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iy1dZ-XUJHsF",
    "outputId": "45f9689d-8bc4-4b6f-bc15-5af2c54d9141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "#set random seeds\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#machine learning\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#accessing files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "#display charts/images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#don't need\n",
    "# from tensorflow.python.keras.preprocessing import sequence\n",
    "# from tensorflow.python.keras.preprocessing import text\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ig0i-YXvJHsO"
   },
   "outputs": [],
   "source": [
    "params={\n",
    "    'image_size': [256, 256],\n",
    "    'vocab_size': 10000,\n",
    "    'text_input_length': 49,\n",
    "    'nodes': 256,\n",
    "    'tokenizer_start_index': 58, #index of tokenizer to signal sequence start\n",
    "    'tokenizer_end_index': 57,\n",
    "    'epochs': 15,\n",
    "    'version': 1,\n",
    "    'embedding_dim': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0QY626PJHsU"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    credentials=None\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "\n",
    "\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file( #file location of GCS private key\n",
    "        '/Users/jeremiahherberg/Downloads/hateful-memes-af65c70c1b79.json')\n",
    "\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qa0ralXEJHsX"
   },
   "outputs": [],
   "source": [
    "bucket = 'jh_coco_2014'\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)\n",
    "objects = client.list_blobs(bucket, prefix='coco2014')\n",
    "tfrecords = []\n",
    "for object_ in objects:\n",
    "    path = str(object_).split(', ')[1]\n",
    "    gs_path = os.path.join('gs://', bucket, path)\n",
    "    tfrecords.append(gs_path) #gs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "k3pADpk0JHsa",
    "outputId": "b6df9959-4168-47f4-bbd3-6c4884a1a7a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://jh_coco_2014/coco2014_1_of_7.tfrecord',\n",
       " 'gs://jh_coco_2014/coco2014_2_of_7.tfrecord',\n",
       " 'gs://jh_coco_2014/coco2014_3_of_7.tfrecord',\n",
       " 'gs://jh_coco_2014/coco2014_4_of_7.tfrecord',\n",
       " 'gs://jh_coco_2014/coco2014_5_of_7.tfrecord',\n",
       " 'gs://jh_coco_2014/coco2014_6_of_7.tfrecord',\n",
       " 'gs://jh_coco_2014/coco2014_7_of_7.tfrecord']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6t9Kv3FyJHse",
    "outputId": "6ffb2780-b9f6-4800-c253-04c0ab794a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arYwtpLjJHsi"
   },
   "outputs": [],
   "source": [
    "def decode_example(example):\n",
    "    '''\n",
    "    decodes single tfexample from TFrecord file\n",
    "    '''\n",
    "    features = {'text': tf.io.FixedLenFeature([], tf.string),\n",
    "                'image': tf.io.FixedLenFeature([], tf.string),\n",
    "                'raw_image': tf.io.FixedLenFeature([], tf.string)}\n",
    "    single_example = tf.io.parse_single_example(example, features)\n",
    "    \n",
    "    text = tf.io.parse_tensor(single_example['text'], out_type=tf.int32)\n",
    "    text = tf.cast(text, tf.float32) \n",
    "    image_features = tf.io.parse_tensor(single_example['image'], out_type=tf.float32)\n",
    "    image = tf.io.decode_jpeg(single_example['raw_image'], 3)\n",
    "    image = tf.image.resize_with_pad(image, *params['image_size'])\n",
    "    image = image / 255.0\n",
    "    # label = tf.cast(label, tf.float32)\n",
    "    return image_features, text\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWKwh9FxouLL"
   },
   "outputs": [],
   "source": [
    "def combine(image, text):\n",
    "    '''\n",
    "    todo - work on documentation\n",
    "    '''\n",
    "    WORDS = tf.math.count_nonzero(text, dtype=tf.int32)\n",
    "    COUNTER = tf.constant(0, dtype=tf.int32)\n",
    "    y  = tf.reshape(text[:,1:WORDS], (-1,1)) #basically free\n",
    "\n",
    "    initial_Xtext = tf.zeros((1, 49))\n",
    "    initial_Xtext = tf.concat([initial_Xtext[:, :COUNTER], \n",
    "                               text[:, COUNTER:COUNTER+1], \n",
    "                               initial_Xtext[:, COUNTER+1:]], axis=-1)\n",
    "    \n",
    "    def condition(counter, img, img2, txt, ini_text, text_out, words ):\n",
    "        return tf.less(counter, words - 2) #2 less than text seq len\n",
    "    \n",
    "    def body(counter, img, img2, txt, ini_text, text_out, words):\n",
    "        \n",
    "#         img = img #clean up img and img2 \n",
    "        img = tf.concat([img, img2], axis=0) #this can be returned unchanged\n",
    "\n",
    "        counter = tf.add(counter, 1) #add +1 to counter\n",
    "\n",
    "\n",
    "        ini_text = tf.concat([ini_text[:, :counter], \n",
    "                              txt[:, counter:counter+1], \n",
    "                              ini_text[:, counter+1:]], axis=-1)\n",
    "        text_out = tf.concat([text_out, ini_text], axis=0)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        return counter, img, img2, txt, ini_text, text_out, words\n",
    "    _, image, _, _, _, txt_out, words= tf.while_loop(condition, \n",
    "                                                     body, \n",
    "                                                     [COUNTER, image, \n",
    "                                                      image, text, \n",
    "                                                      initial_Xtext, initial_Xtext,\n",
    "                                                      WORDS])\n",
    "    return image, txt_out, y, words\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVGvXnmBJHsk"
   },
   "outputs": [],
   "source": [
    "def create_ds(files, params):\n",
    "    '''\n",
    "    function to create dataset for training/validation\n",
    "    \n",
    "    args:\n",
    "        files: list of str, filepaths of TFrecord files to be used in DS\n",
    "        params: dict with the following keys:\n",
    "            batch_size: int, batch size of training/validation step\n",
    "            examples_per_file: int, number of examples in each TFrecord file\n",
    "        train, bool, default True, indicator if the DS is for training\n",
    "        test_examples, int: default 1000 number of examples in test dataset\n",
    "    returns:\n",
    "        ds: tensorflow input pipeline with images, text and labels\n",
    "            output of ds is: (text, image), label\n",
    "        ds_batches: int, number of steps in each epoch based on the batch_size\n",
    "    '''\n",
    "    batch_size = 1\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(filenames = files)\n",
    "    ds = ds.map(decode_example, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # ds = ds.map(combine)\n",
    "\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "#     ds = ds.cache() \n",
    "    \n",
    "    return ds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKHCPu3cZf57"
   },
   "outputs": [],
   "source": [
    "def download_file(client, bucket, file_name):\n",
    "    '''\n",
    "    downloads a file from a GCS bucket into working directory\n",
    "\n",
    "    args:\n",
    "        client: google.cloud.storage.Client object\n",
    "        bucket: str, name of bucket to download file from\n",
    "        file_name: str, file name to download\n",
    "    returns: None\n",
    "    \n",
    "    '''\n",
    "    _bucket = client.bucket(bucket)\n",
    "    blob = _bucket.blob(file_name)\n",
    "    blob.download_to_filename(file_name)\n",
    "\n",
    "def create_tokenizer_from_filename(file_name,\n",
    "                                  client=None,\n",
    "                                  bucket=None):\n",
    "    '''\n",
    "    creates tf.keras.preprocessing.text.tokenizer from a \n",
    "    json config file in current working directory\n",
    "    args:\n",
    "        file_name: str, filename where config json file is located\n",
    "        client, google.cloud.storage.Client object, default None, if an arg\n",
    "            is passed, function will first check if glove_file exists in current\n",
    "            directory, and if not, will download an object located at glove_file\n",
    "            in the bucket passed into bucket arg\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file\n",
    "    returns:\n",
    "        tokenizer object\n",
    "    '''\n",
    "    if client:\n",
    "        if not os.path.isfile(file_name):\n",
    "            download_file(client, bucket,file_name)\n",
    "    with open(file_name) as file:\n",
    "        open_file = json.load(file)\n",
    "        tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(open_file)\n",
    "    return tokenizer\n",
    "\n",
    "def get_embedding_weights_from_tokenizer_glove(glove_file,\n",
    "                                              tokenizer,\n",
    "                                              embedding_dim,\n",
    "                                              client=None,\n",
    "                                              bucket=None,\n",
    "                                              ):\n",
    "    '''\n",
    "    gets the weights to use in an embedding layer from a pretained\n",
    "    model based on the tokenizer used to create sequences that will\n",
    "    be passed into embedding layer\n",
    "    \n",
    "    args:\n",
    "        glove_file: str, path of pretrained model from current directory\n",
    "        tokenizer: tf.keras.preprocessing.text.tokenizer object, tokenizer\n",
    "            that was used to create sequences\n",
    "        embedding_dim: int, output_dim of embedding layer of pre-trained model\n",
    "        client, google.cloud.storage.Client object, default None, if an arg\n",
    "            is passed, function will first check if glove_file exists in current\n",
    "            directory, and if not, will download an object located at glove_file\n",
    "            in the bucket passed into bucket arg\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file\n",
    "    returns: \n",
    "        embedding_weights: numpy array, shaped* (vocab_size, embedding_dim)\n",
    "            weights that can be used for embedding layer\n",
    "            *vocab_size = tokenizer.num_words which is the number of words in\n",
    "            the tokenizer vocabulary\n",
    "        \n",
    "    '''\n",
    "    if client:\n",
    "        if not os.path.isfile(glove_file):\n",
    "            download_file(client, bucket, glove_file)\n",
    "    word_values = dict()\n",
    "    file = open(glove_file, encoding='utf-8')\n",
    "    \n",
    "    for line in file:\n",
    "        coeff = line.split()\n",
    "        word = coeff[0]\n",
    "        coefficients = np.asarray(coeff[-300:], dtype='float32')\n",
    "        word_values[word] = coefficients\n",
    "    file.close()\n",
    "    vocab_size = tokenizer.num_words\n",
    "    embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx < vocab_size:\n",
    "            word_embedding_values = word_values.get(word)\n",
    "            if word_embedding_values is not None:\n",
    "                embedding_weights[idx] = word_embedding_values\n",
    "    \n",
    "    return embedding_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kx474nk2JHsm"
   },
   "outputs": [],
   "source": [
    "ds = create_ds(tfrecords, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKb5yx6PJHsr"
   },
   "outputs": [],
   "source": [
    "def create_model(params, embedding_weights):\n",
    "    '''\n",
    "    creates model to caption images\n",
    "    '''\n",
    "    vocab_size = params['vocab_size']\n",
    "    txt_input_length = params['text_input_length']\n",
    "    nodes = params['nodes']\n",
    "    embedding_dim = params['embedding_dim']\n",
    "\n",
    "    image_feature_inp = layers.Input((64, 2048), name='features_input')\n",
    "    features = layers.Flatten()(image_feature_inp)\n",
    "    features = layers.Dense(nodes, activation='relu')(features)\n",
    "    \n",
    "    txt_inp = layers.Input((txt_input_length,), name='text_input')\n",
    "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(txt_inp)\n",
    "    sequences = layers.LSTM(nodes)(embedding)\n",
    "\n",
    "    features_tst = features #layers.LSTM(nodes)(features) #not sure if this is appropiate\n",
    "    #if above doesn't work well, flatten after image_feature_imp\n",
    "    decoder = layers.Add()([features_tst, sequences])\n",
    "    decoder = layers.Dense(nodes, activation='relu')(decoder)\n",
    "    output = layers.Dense(vocab_size, activation='softmax')(decoder)\n",
    "    model = keras.Model([image_feature_inp, txt_inp], output)\n",
    "    model.layers[3].set_weights([embedding_weights])\n",
    "    model.layers[3].trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8DlDjMDZf6A"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer_from_filename('coco_tokenizer.json', \n",
    "                                           client,\n",
    "                                           'jh_coco_2014')\n",
    "embedding_weights = get_embedding_weights_from_tokenizer_glove('glove.840B.300d.txt',\n",
    "                                                               tokenizer,\n",
    "                                                               300,\n",
    "                                                               client,\n",
    "                                                               'jh_hateful_memes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVGtPCLSJHst"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = create_model(params, embedding_weights)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_tracker = tf.keras.metrics.Mean(name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KnC_gdymcvT7"
   },
   "outputs": [],
   "source": [
    "# model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYB3RgKvJHsv"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    '''\n",
    "    taken from https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "    '''\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    loss = tf.reduce_mean(loss_)\n",
    "    #update loss tracker\n",
    "    loss_tracker.update_state(loss)\n",
    "\n",
    "    return loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIaBARmZJHsx"
   },
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_step(image_, text_, y, y_len): # params\n",
    "    '''\n",
    "    todo - work on documentation\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    def step(image, text, y_value, y_len_):\n",
    "        \n",
    "        img_tmp = image\n",
    "#         for _ in range(y_len_): \n",
    "#             image = tf.concat([image, img_tmp], axis=0)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model((image, text), training=True)\n",
    "            loss = loss_function(y_value, preds)\n",
    "            trainable_variables = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        # return preds, loss\n",
    "    #preds, loss = \n",
    "    strategy.run(step, args=(image_, text_, y, y_len))\n",
    "    # return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99PbKbzsNDoy"
   },
   "outputs": [],
   "source": [
    "epochs = params['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3FjLQMcrZf6I",
    "outputId": "984f79fc-d328-46d5-9e2b-14e46f1de310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step:10000, loss: 4.50011, batch time:359.496\n",
      "epoch: 1, step:20000, loss: 4.19932, batch time:710.719\n",
      "epoch: 1, step:30000, loss: 4.07132, batch time:1054.567\n",
      "epoch: 1, step:40000, loss: 3.99824, batch time:1410.751\n",
      "epoch: 1, step:50000, loss: 3.95309, batch time:1758.996\n",
      "epoch: 1, step:60000, loss: 3.89708, batch time:2093.025\n",
      "epoch:1, loss:3.87875, time:2447.732, steps:69962\n",
      "epoch: 2, step:10000, loss: 6.84639, batch time:320.466\n",
      "epoch: 2, step:20000, loss: 5.26412, batch time:665.019\n",
      "epoch: 2, step:30000, loss: 4.72918, batch time:983.163\n",
      "epoch: 2, step:40000, loss: 4.44133, batch time:1319.924\n",
      "epoch: 2, step:50000, loss: 4.25317, batch time:1653.824\n",
      "epoch: 2, step:60000, loss: 4.11401, batch time:1983.797\n",
      "epoch:2, loss:4.03174, time:2317.720, steps:69962\n",
      "epoch: 3, step:10000, loss: 3.74329, batch time:322.744\n",
      "epoch: 3, step:20000, loss: 3.64490, batch time:646.677\n",
      "epoch: 3, step:30000, loss: 3.61835, batch time:990.171\n",
      "epoch: 3, step:40000, loss: 3.58385, batch time:1312.051\n",
      "epoch: 3, step:50000, loss: 3.54674, batch time:1616.877\n",
      "epoch: 3, step:60000, loss: 3.51085, batch time:1928.872\n",
      "epoch:3, loss:3.50353, time:2241.339, steps:69962\n",
      "epoch: 4, step:10000, loss: 3.64397, batch time:329.433\n",
      "epoch: 4, step:20000, loss: 3.55551, batch time:640.246\n",
      "epoch: 4, step:30000, loss: 3.53890, batch time:967.818\n",
      "epoch: 4, step:40000, loss: 3.50943, batch time:1267.305\n",
      "epoch: 4, step:50000, loss: 3.47776, batch time:1563.545\n",
      "epoch: 4, step:60000, loss: 3.44231, batch time:1859.411\n",
      "epoch:4, loss:3.43817, time:2153.439, steps:69962\n",
      "epoch: 5, step:10000, loss: 3.60131, batch time:303.651\n",
      "epoch: 5, step:20000, loss: 3.52858, batch time:615.932\n",
      "epoch: 5, step:30000, loss: 3.51439, batch time:942.463\n",
      "epoch: 5, step:40000, loss: 3.48290, batch time:1239.931\n",
      "epoch: 5, step:50000, loss: 3.45054, batch time:1533.492\n",
      "epoch: 5, step:60000, loss: 3.41660, batch time:1826.531\n",
      "epoch:5, loss:3.41537, time:2114.373, steps:69962\n",
      "epoch: 6, step:10000, loss: 3.55879, batch time:296.406\n",
      "epoch: 6, step:20000, loss: 3.47768, batch time:590.385\n",
      "epoch: 6, step:30000, loss: 3.46304, batch time:889.355\n",
      "epoch: 6, step:40000, loss: 3.43873, batch time:1176.681\n",
      "epoch: 6, step:50000, loss: 3.41526, batch time:1454.523\n",
      "epoch: 6, step:60000, loss: 3.38628, batch time:1735.391\n",
      "epoch:6, loss:3.38629, time:2025.074, steps:69962\n",
      "epoch: 7, step:10000, loss: 3.56326, batch time:283.231\n",
      "epoch: 7, step:20000, loss: 3.48174, batch time:564.916\n",
      "epoch: 7, step:30000, loss: 3.47257, batch time:845.934\n",
      "epoch: 7, step:40000, loss: 3.45168, batch time:1129.649\n",
      "epoch: 7, step:50000, loss: 3.43489, batch time:1408.556\n",
      "epoch: 7, step:60000, loss: 3.40964, batch time:1684.415\n",
      "epoch:7, loss:3.41260, time:1966.544, steps:69962\n",
      "epoch: 8, step:10000, loss: 3.63498, batch time:298.240\n",
      "epoch: 8, step:20000, loss: 3.53919, batch time:586.302\n",
      "epoch: 8, step:30000, loss: 3.52474, batch time:868.072\n",
      "epoch: 8, step:40000, loss: 3.50338, batch time:1147.660\n",
      "epoch: 8, step:50000, loss: 3.47892, batch time:1425.355\n",
      "epoch: 8, step:60000, loss: 3.44505, batch time:1700.836\n",
      "epoch:8, loss:3.44354, time:1978.963, steps:69962\n",
      "epoch: 9, step:10000, loss: 3.64253, batch time:287.142\n",
      "epoch: 9, step:20000, loss: 3.55521, batch time:571.470\n",
      "epoch: 9, step:30000, loss: 3.53990, batch time:827.237\n",
      "epoch: 9, step:40000, loss: 3.51151, batch time:1092.135\n",
      "epoch: 9, step:50000, loss: 3.48537, batch time:1351.103\n",
      "epoch: 9, step:60000, loss: 3.45042, batch time:1606.461\n",
      "epoch:9, loss:3.44875, time:1862.213, steps:69962\n",
      "epoch: 10, step:10000, loss: 3.64509, batch time:283.229\n",
      "epoch: 10, step:20000, loss: 3.56042, batch time:558.896\n",
      "epoch: 10, step:30000, loss: 3.54426, batch time:833.973\n",
      "epoch: 10, step:40000, loss: 3.51405, batch time:1111.260\n",
      "epoch: 10, step:50000, loss: 3.48966, batch time:1384.444\n",
      "epoch: 10, step:60000, loss: 3.45425, batch time:1655.322\n",
      "epoch:10, loss:3.45286, time:1928.080, steps:69962\n",
      "epoch: 11, step:10000, loss: 3.64558, batch time:281.471\n",
      "epoch: 11, step:20000, loss: 3.56909, batch time:564.000\n",
      "epoch: 11, step:30000, loss: 3.55639, batch time:843.425\n",
      "epoch: 11, step:40000, loss: 3.52864, batch time:1120.186\n",
      "epoch: 11, step:50000, loss: 3.50252, batch time:1386.201\n",
      "epoch: 11, step:60000, loss: 3.46636, batch time:1645.636\n",
      "epoch:11, loss:3.46300, time:1899.281, steps:69962\n",
      "epoch: 12, step:10000, loss: 3.64693, batch time:270.610\n",
      "epoch: 12, step:20000, loss: 3.57581, batch time:532.398\n",
      "epoch: 12, step:30000, loss: 3.56994, batch time:800.004\n",
      "epoch: 12, step:40000, loss: 3.54149, batch time:1084.359\n",
      "epoch: 12, step:50000, loss: 3.51610, batch time:1352.190\n",
      "epoch: 12, step:60000, loss: 3.48129, batch time:1620.833\n",
      "epoch:12, loss:3.47834, time:1903.585, steps:69962\n",
      "epoch: 13, step:10000, loss: 3.66690, batch time:272.590\n",
      "epoch: 13, step:20000, loss: 3.59818, batch time:558.241\n",
      "epoch: 13, step:30000, loss: 3.59070, batch time:855.293\n",
      "epoch: 13, step:40000, loss: 3.56120, batch time:1153.063\n",
      "epoch: 13, step:50000, loss: 3.52988, batch time:1436.451\n",
      "epoch: 13, step:60000, loss: 3.49417, batch time:1712.054\n",
      "epoch:13, loss:3.49181, time:1964.573, steps:69962\n",
      "epoch: 14, step:10000, loss: 3.69342, batch time:274.709\n",
      "epoch: 14, step:20000, loss: 3.62999, batch time:542.962\n",
      "epoch: 14, step:30000, loss: 3.62631, batch time:813.780\n",
      "epoch: 14, step:40000, loss: 3.60550, batch time:1089.846\n",
      "epoch: 14, step:50000, loss: 3.57952, batch time:1359.093\n",
      "epoch: 14, step:60000, loss: 3.54414, batch time:1618.874\n",
      "epoch:14, loss:3.54039, time:1885.648, steps:69962\n",
      "epoch: 15, step:10000, loss: 3.72022, batch time:287.658\n",
      "epoch: 15, step:20000, loss: 3.64923, batch time:568.320\n",
      "epoch: 15, step:30000, loss: 3.63795, batch time:837.030\n",
      "epoch: 15, step:40000, loss: 3.60809, batch time:1104.379\n",
      "epoch: 15, step:50000, loss: 3.57938, batch time:1373.445\n",
      "epoch: 15, step:60000, loss: 3.54580, batch time:1629.830\n",
      "epoch:15, loss:3.54310, time:1893.216, steps:69962\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    step = 0\n",
    "    for image, text in ds:\n",
    "        img, txt, y, y_len = combine(image, text)\n",
    "        _ = train_step(img, txt, y, y_len)\n",
    "        step +=1 \n",
    "        if step % 10000 == 0:\n",
    "            batch_time = time.time()\n",
    "            time_batch = batch_time - epoch_start\n",
    "            print('epoch: {}, step:{}, loss: {:.5f}, batch time:{:.3f}'.format(epoch +1,\n",
    "                                                                      step,\n",
    "                                                                      loss_tracker.result().numpy(),\n",
    "                                                                      time_batch))\n",
    "    batch_time = time.time()\n",
    "    time_batch = batch_time - epoch_start\n",
    "    print('epoch:{}, loss:{:.5f}, time:{:.3f}, steps:{}'.format(epoch+1, \n",
    "                                                                loss_tracker.result().numpy(), \n",
    "                                                                time_batch, step))\n",
    "    loss_tracker.reset_states()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5I0r55eZf6J"
   },
   "outputs": [],
   "source": [
    "model_num = params['version']\n",
    "model_path = 'image_caption_model_v{}.h5'.format(model_num)\n",
    "model.save(model_path)\n",
    "model_bucket = client.bucket('jh_hateful_memes')\n",
    "blob = model_bucket.blob(model_path)\n",
    "blob.upload_from_filename(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ea2kjWldUB6v"
   },
   "outputs": [],
   "source": [
    "def inference(params, image_feature, image, model, tokenizer):\n",
    "    '''\n",
    "    uses an image captioning model to generate a caption of an image\n",
    "    \n",
    "    args:\n",
    "        params: dictionary with at least the following keys:\n",
    "            tokenizer_start_index: int, tokenizer value that signals start\n",
    "            of caption\n",
    "            tokenizer_end_index: int, tokenizer value that signals end of\n",
    "            caption\n",
    "            text_input_length: int, len of text input of model\n",
    "        image_feature: array, shaped (1, 64, 2048) output of an image being\n",
    "            passed through InceptionV3 model without classification layer\n",
    "        model: tensorflow functional model, model to generate caption\n",
    "        tokenizer: tf.keras.preprocessing.text.tokenizer object, \n",
    "    '''\n",
    "    text_len = params['text_input_length']\n",
    "    text = np.zeros((1, text_len))\n",
    "    results = list()\n",
    "    result = params['tokenizer_start_index']\n",
    "    for idx in range(text_len):\n",
    "        text[:, idx] = result\n",
    "        result = model((image_features, text))\n",
    "        result = tf.argmax(result[0]).numpy()\n",
    "        if result == params['tokenizer_end_index']:\n",
    "            break\n",
    "        results.append(result)\n",
    "    results_converted = tokenizer.sequences_to_texts([results])[0]\n",
    "    print(results_converted)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "image_caption_model (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
