{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FjLO4aSAPmtS"
   },
   "outputs": [],
   "source": [
    "#download glove model from http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip and\n",
    "#upload to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4HlT_xi_PqVy",
    "outputId": "a5e66d62-7683-4ede-ed6e-f83584e0d519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "#set random seeds\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#machine learning\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#accessing files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "#display charts/images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#don't need\n",
    "# from tensorflow.python.keras.preprocessing import sequence\n",
    "# from tensorflow.python.keras.preprocessing import text\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hHdRotMXPvzf"
   },
   "outputs": [],
   "source": [
    "params={\n",
    "    'image_size': [256, 256],\n",
    "    'vocab_size': 10000,\n",
    "    'text_input_length': 49,\n",
    "    'nodes': 256,\n",
    "    'tokenizer_start_index': 58, #index of tokenizer to signal sequence start\n",
    "    'tokenizer_end_index': 57,\n",
    "    'epochs': 5000,\n",
    "    'version': 5,\n",
    "    'embedding_dim': 300,\n",
    "    'ds_size': 801592\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bVoXmMZ-PzBE"
   },
   "outputs": [],
   "source": [
    "training_bucket = 'gs://kds-e7996502fe373b391a0a14641ad5f932ee7d607744dbe970cc8ffe08'\n",
    "glove_bucket = 'gs://kds-5123f8991f380aa8ec3a0dfae64a3732b529d4e504450dd8f9e55fb1'\n",
    "tfrecords = tf.io.gfile.glob(training_bucket + '/*tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jyaqUKlxP6sO",
    "outputId": "c159ba5b-dd8c-4feb-d547-83ed7c52c7e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://kds-5123f8991f380aa8ec3a0dfae64a3732b529d4e504450dd8f9e55fb1/glove.840B.300d.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.io.gfile.glob(glove_bucket + '/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "id": "ItHeNdCWP8c8",
    "outputId": "fa207f7b-4bd9-41e2-fb20-05f718857992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.94.61.178:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.94.61.178:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.94.61.178:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i4jSZCTqP_Sl"
   },
   "outputs": [],
   "source": [
    "def decode_example(example):\n",
    "    '''\n",
    "    decodes single tfexample from TFrecord file\n",
    "    '''\n",
    "    features = {'text': tf.io.FixedLenFeature([], tf.string),\n",
    "                'inception': tf.io.FixedLenFeature([], tf.string), #can also be vgg\n",
    "                'y': tf.io.FixedLenFeature([], tf.string)}\n",
    "    single_example = tf.io.parse_single_example(example, features)\n",
    "    \n",
    "    text = tf.io.parse_tensor(single_example['text'], out_type=tf.int32)\n",
    "    text = tf.cast(text, tf.float32)\n",
    "    image_features = tf.io.parse_tensor(single_example['inception'], out_type=tf.float32)\n",
    "    y_value = tf.io.parse_tensor(single_example['y'], out_type=tf.int32)\n",
    "    y_value = tf.expand_dims(y_value, axis=0)\n",
    "\n",
    "    return (image_features, text), y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "__13dtuKQCHs"
   },
   "outputs": [],
   "source": [
    "def create_ds(files, params):\n",
    "    '''\n",
    "    function to create dataset for training/validation\n",
    "    \n",
    "    args:\n",
    "        files: list of str, filepaths of TFrecord files to be used in DS\n",
    "        params: dict with the following keys:\n",
    "            batch_size: int, batch size of training/validation step\n",
    "            examples_per_file: int, number of examples in each TFrecord file\n",
    "        train, bool, default True, indicator if the DS is for training\n",
    "        test_examples, int: default 1000 number of examples in test dataset\n",
    "    returns:\n",
    "        ds: tensorflow input pipeline with images, text and labels\n",
    "            output of ds is: (text, image), label\n",
    "        ds_batches: int, number of steps in each epoch based on the batch_size\n",
    "    '''\n",
    "#     batch_size = 801592\n",
    "    batch_size = params['ds_size']\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(filenames = files)\n",
    "    ds = ds.map(decode_example, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "#     ds = ds.cache() \n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "13sPzX8PQEWr"
   },
   "outputs": [],
   "source": [
    "def download_file(bucket, file_name):\n",
    "    '''\n",
    "    downloads a file from a public GCS bucket into working directory\n",
    "\n",
    "    args:\n",
    "        bucket: str, name of bucket to download file from\n",
    "        file_name: str, file name to download\n",
    "    returns: None\n",
    "    \n",
    "    '''\n",
    "    file_path = tf.io.gfile.glob(bucket + '/' + file_name)[0]\n",
    "    tf.io.gfile.copy(file_path, file_name)\n",
    "\n",
    "def create_tokenizer_from_filename(file_name,\n",
    "                                  bucket=None):\n",
    "    '''\n",
    "    creates tf.keras.preprocessing.text.tokenizer from a \n",
    "    json config file in current working directory\n",
    "    args:\n",
    "        file_name: str, filename where config json file is located\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file, if an arg\n",
    "            is passed, function will first check if file_name exists in current\n",
    "            directory, and if not, will download an object located at file_name\n",
    "            in the bucket passed into bucket arg\n",
    "    returns:\n",
    "        tokenizer object\n",
    "    '''\n",
    "    if bucket:\n",
    "        if not os.path.isfile(file_name):\n",
    "            download_file(bucket,file_name)\n",
    "    with open(file_name) as file:\n",
    "        open_file = json.load(file)\n",
    "        tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(open_file)\n",
    "    return tokenizer\n",
    "\n",
    "def get_embedding_weights_from_tokenizer_glove(glove_file,\n",
    "                                              tokenizer,\n",
    "                                              embedding_dim,\n",
    "                                              bucket=None,\n",
    "                                              ):\n",
    "    '''\n",
    "    gets the weights to use in an embedding layer from a pretained\n",
    "    model based on the tokenizer used to create sequences that will\n",
    "    be passed into embedding layer\n",
    "    \n",
    "    args:\n",
    "        glove_file: str, path of pretrained model from current directory\n",
    "        tokenizer: tf.keras.preprocessing.text.tokenizer object, tokenizer\n",
    "            that was used to create sequences\n",
    "        embedding_dim: int, output_dim of embedding layer of pre-trained model\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file, if an arg\n",
    "            is passed, function will first check if glove_file exists in current\n",
    "            directory, and if not, will download an object located at glove_file\n",
    "            in the bucket passed into bucket arg\n",
    "    returns: \n",
    "        embedding_weights: numpy array, shaped* (vocab_size, embedding_dim)\n",
    "            weights that can be used for embedding layer\n",
    "            *vocab_size = tokenizer.num_words which is the number of words in\n",
    "            the tokenizer vocabulary\n",
    "        \n",
    "    '''\n",
    "    if bucket:\n",
    "        if not os.path.isfile(glove_file):\n",
    "            download_file(bucket, glove_file)\n",
    "    word_values = dict()\n",
    "    file = open(glove_file, encoding='utf-8')\n",
    "    \n",
    "    for line in file:\n",
    "        coeff = line.split()\n",
    "        word = coeff[0]\n",
    "        coefficients = np.asarray(coeff[-300:], dtype='float32')\n",
    "        word_values[word] = coefficients\n",
    "    file.close()\n",
    "    vocab_size = tokenizer.num_words\n",
    "    embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx < vocab_size:\n",
    "            word_embedding_values = word_values.get(word)\n",
    "            if word_embedding_values is not None:\n",
    "                embedding_weights[idx] = word_embedding_values\n",
    "    \n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kJWEIo7jQHtr"
   },
   "outputs": [],
   "source": [
    "ds = create_ds(tfrecords, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oV8rX4WiQKK-"
   },
   "outputs": [],
   "source": [
    "def create_model(params, embedding_weights=None):\n",
    "    '''\n",
    "    creates model to caption images\n",
    "    '''\n",
    "    vocab_size = params['vocab_size']\n",
    "    txt_input_length = params['text_input_length']\n",
    "    nodes = params['nodes']\n",
    "    embedding_dim = params['embedding_dim']\n",
    "\n",
    "    image_feature_inp = layers.Input((2048,), name='features_input')\n",
    "    features = layers.Dropout(0.5)(image_feature_inp)\n",
    "    features = layers.Dense(nodes)(features)\n",
    "    features = layers.ReLU()(features)\n",
    "    \n",
    "    txt_inp = layers.Input((txt_input_length,), name='text_input')\n",
    "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(txt_inp)\n",
    "    embedding = layers.Dropout(0.5)(embedding)\n",
    "    sequences = layers.GRU(nodes)(embedding)\n",
    "\n",
    "    decoder = layers.Add()([features, sequences]) # Concatenate - try\n",
    "    decoder = layers.Dense(nodes, activation=None)(decoder)\n",
    "    decoder = layers.ReLU()(decoder)\n",
    "    output = layers.Dense(vocab_size, activation='softmax')(decoder)\n",
    "    model = keras.Model([image_feature_inp, txt_inp], output)\n",
    "    model.layers[3].set_weights([embedding_weights])\n",
    "    model.layers[3].trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2stxJDfZQO9K"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer_from_filename('coco_tokenizer.json', \n",
    "                                           training_bucket)\n",
    "embedding_weights = get_embedding_weights_from_tokenizer_glove('glove.840B.300d.txt',\n",
    "                                                               tokenizer,\n",
    "                                                               300,\n",
    "                                                               glove_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rc14ogWHQRsb"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = create_model(params, embedding_weights) #embedding_weights\n",
    "    model.compile(optimizer='adam', loss='SparseCategoricalCrossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B908sv1AQxjG",
    "outputId": "6d5b0042-8756-4d29-bc03-718119e9be80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  1/150 [..............................] - ETA: 6:50 - loss: 9.2213WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0375s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0375s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 11s 70ms/step - loss: 4.7383\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 5s 37ms/step - loss: 3.3999\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 3.0614\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.8873\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.7752\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.6900\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.6236\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.5698\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.5205\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.4786\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.4440\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.4120\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.3818\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.3549\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.3305\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.3091\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.2870\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.2699\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.2527\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.2372\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.2213\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.2060\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.1939\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.1818\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.1696\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.1584\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.1473\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.1392\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.1305\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.1217\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.1129\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.1061\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0971\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0904\n",
      "Epoch 35/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0840\n",
      "Epoch 36/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0775\n",
      "Epoch 37/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.0716\n",
      "Epoch 38/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.0661\n",
      "Epoch 39/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0595\n",
      "Epoch 40/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0543\n",
      "Epoch 41/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0490\n",
      "Epoch 42/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0430\n",
      "Epoch 43/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.0386\n",
      "Epoch 44/50\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.0341\n",
      "Epoch 45/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0307\n",
      "Epoch 46/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0254\n",
      "Epoch 47/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0217\n",
      "Epoch 48/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0172\n",
      "Epoch 49/50\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 2.0145\n",
      "Epoch 50/50\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 2.0103\n"
     ]
    }
   ],
   "source": [
    "for input_data, ground_truth in ds:\n",
    "    history = model.fit(input_data, ground_truth, steps_per_epoch=150, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "v_pvJBLBQ8Sw",
    "outputId": "59cdf590-104c-4914-b1b2-8ca458dc949c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    credentials=None\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "\n",
    "\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file( #file location of GCS private key\n",
    "        '/Users/jeremiahherberg/Downloads/hateful-memes-af65c70c1b79.json')\n",
    "\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "i00IxUEqQ_XB"
   },
   "outputs": [],
   "source": [
    "model_num = params['version']\n",
    "model_path = 'image_caption_model_v{}.h5'.format(model_num)\n",
    "model.save(model_path)\n",
    "model_bucket = client.bucket('jh_hateful_memes')\n",
    "blob = model_bucket.blob(model_path)\n",
    "blob.upload_from_filename(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XMkR9-LyRCrp"
   },
   "outputs": [],
   "source": [
    "def get_image_captions(params, images):\n",
    "    '''\n",
    "    creates captions to a group of images\n",
    "    \n",
    "    args:\n",
    "        params: dictionary with at least the following keys:\n",
    "            caption_text_input_length: int, length of captions\n",
    "            tokenizer_start_index: int, value to signal start of caption\n",
    "            tokenizer_end_index: int, value to signal end of caption\n",
    "            \n",
    "        images: tensor, dtype: tf.float32 shaped (None, 299, 299, 3) None is the \n",
    "        number of images, each image should be normalized to have\n",
    "        pixel values of -1 to 1. Images to be captioned\n",
    "\n",
    "            \n",
    "    returns:\n",
    "        captions: tensor, dtype float, shaped \n",
    "        (None, params['caption_text_input_length'])None is the number of \n",
    "        images, image caption sequences\n",
    "    '''\n",
    "    num_images = len(images)\n",
    "    caption_len = params['text_input_length']\n",
    "    caption_end_index = params['tokenizer_end_index']\n",
    "\n",
    "    @tf.function\n",
    "    def get_capt(img, txt):\n",
    "        def caption_step(image_, text_):\n",
    "            '''\n",
    "            evaluate model here\n",
    "            '''\n",
    "            txt_ = tf.expand_dims(text_, axis=0)\n",
    "            pred = model((image_, txt_))\n",
    "\n",
    "\n",
    "            return pred\n",
    "        result = strategy.run(caption_step, args=(img, txt))\n",
    "        return result\n",
    "\n",
    "    \n",
    "    captions = list()\n",
    "    for image in range(num_images):\n",
    "        img = images[image]\n",
    "        img = tf.expand_dims(img, axis=0)\n",
    "        txt_input = np.zeros((caption_len))\n",
    "        result = params['tokenizer_start_index']\n",
    "        for idx in range(caption_len):\n",
    "            txt_input[idx] = result\n",
    "            # with tf.device('/TPU:0'):\n",
    "            #     result = caption_step(img, txt_input)\n",
    "                # result = strategy.run(caption_step, args=(img, txt_input))\n",
    "            result = get_capt(img, txt_input)\n",
    "            result = result.numpy()[0] # result.values[0].numpy()[0]\n",
    "            result = tf.argmax(result, axis=0)\n",
    "            if result == caption_end_index:\n",
    "                break\n",
    "        captions.append(txt_input)\n",
    "    captions = tf.convert_to_tensor(captions)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Rj30lDCjRITk"
   },
   "outputs": [],
   "source": [
    "def plot_metric(metric1, ylabel):\n",
    "    plt.plot(history.history[metric1], label=metric1)\n",
    "    # plt.plot(history.history[metric2], label=metric2)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "sC14-e95RMSE",
    "outputId": "be0cd92d-92b9-4d26-cc0d-958d7e812ae7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric('loss', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qgLbGy98RPYT",
    "outputId": "65a2e6bc-4bd6-4f7c-9d4d-77a6d07b888c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0102853775024414"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNfKJZsuRR-m"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Untitled25.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
