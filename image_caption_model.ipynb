{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJiMk2hxVVdH"
   },
   "outputs": [],
   "source": [
    "#download glove model from http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip and\n",
    "#upload to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Iy1dZ-XUJHsF",
    "outputId": "81bf8c19-7200-4f66-9888-72de5c20d777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "#set random seeds\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#machine learning\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#accessing files\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "#display charts/images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#don't need\n",
    "# from tensorflow.python.keras.preprocessing import sequence\n",
    "# from tensorflow.python.keras.preprocessing import text\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ig0i-YXvJHsO"
   },
   "outputs": [],
   "source": [
    "params={\n",
    "    'image_size': [256, 256],\n",
    "    'vocab_size': 10000,\n",
    "    'text_input_length': 49,\n",
    "    'nodes': 256,\n",
    "    'tokenizer_start_index': 58, #index of tokenizer to signal sequence start\n",
    "    'tokenizer_end_index': 57,\n",
    "    'epochs': 20,\n",
    "    'version': 7,\n",
    "    'embedding_dim': 300,\n",
    "    'ds_size': 801592,\n",
    "    'batch_size': 5343\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-0QY626PJHsU"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k3pADpk0JHsa"
   },
   "outputs": [],
   "source": [
    "training_bucket = 'gs://kds-e7996502fe373b391a0a14641ad5f932ee7d607744dbe970cc8ffe08'\n",
    "glove_bucket = 'gs://kds-5123f8991f380aa8ec3a0dfae64a3732b529d4e504450dd8f9e55fb1'\n",
    "tfrecords = tf.io.gfile.glob(training_bucket + '/*tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XOOAMRQe7Nob",
    "outputId": "cbf4a7e9-f3d0-43b6-cfda-95b435da4b56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://kds-5123f8991f380aa8ec3a0dfae64a3732b529d4e504450dd8f9e55fb1/glove.840B.300d.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.io.gfile.glob(glove_bucket + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "id": "6t9Kv3FyJHse",
    "outputId": "35ec8fcc-8aa1-418e-f426-593f532f0486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.100.84.234:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.100.84.234:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.100.84.234:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "arYwtpLjJHsi"
   },
   "outputs": [],
   "source": [
    "def decode_example(example):\n",
    "    '''\n",
    "    decodes single tfexample from TFrecord file\n",
    "    '''\n",
    "    features = {'text': tf.io.FixedLenFeature([], tf.string),\n",
    "                'inception': tf.io.FixedLenFeature([], tf.string), #can also be vgg\n",
    "                'y': tf.io.FixedLenFeature([], tf.string)}\n",
    "    single_example = tf.io.parse_single_example(example, features)\n",
    "    \n",
    "    text = tf.io.parse_tensor(single_example['text'], out_type=tf.int32)\n",
    "    text = tf.cast(text, tf.float32)\n",
    "    image_features = tf.io.parse_tensor(single_example['inception'], out_type=tf.float32)\n",
    "    y_value = tf.io.parse_tensor(single_example['y'], out_type=tf.int32)\n",
    "    y_value = tf.expand_dims(y_value, axis=0)\n",
    "\n",
    "    return (image_features, text), y_value\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HVGvXnmBJHsk"
   },
   "outputs": [],
   "source": [
    "def create_ds(files, params):\n",
    "    '''\n",
    "    function to create dataset for training/validation\n",
    "    \n",
    "    args:\n",
    "        files: list of str, filepaths of TFrecord files to be used in DS\n",
    "        params: dict with the following keys:\n",
    "            batch_size: int, batch size of training/validation step\n",
    "            examples_per_file: int, number of examples in each TFrecord file\n",
    "        train, bool, default True, indicator if the DS is for training\n",
    "        test_examples, int: default 1000 number of examples in test dataset\n",
    "    returns:\n",
    "        ds: tensorflow input pipeline with images, text and labels\n",
    "            output of ds is: (text, image), label\n",
    "        ds_batches: int, number of steps in each epoch based on the batch_size\n",
    "    '''\n",
    "    # batch_size = params['batch_size']\n",
    "    batch_size = params['ds_size']\n",
    "\n",
    "    ds = tf.data.TFRecordDataset(filenames = files)\n",
    "    ds = ds.map(decode_example, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "#     ds = ds.cache() \n",
    "    \n",
    "    return ds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NKHCPu3cZf57"
   },
   "outputs": [],
   "source": [
    "def download_file(bucket, file_name):\n",
    "    '''\n",
    "    downloads a file from a public GCS bucket into working directory\n",
    "\n",
    "    args:\n",
    "        bucket: str, name of bucket to download file from\n",
    "        file_name: str, file name to download\n",
    "    returns: None\n",
    "    \n",
    "    '''\n",
    "    file_path = tf.io.gfile.glob(bucket + '/' + file_name)[0]\n",
    "    tf.io.gfile.copy(file_path, file_name)\n",
    "\n",
    "def create_tokenizer_from_filename(file_name,\n",
    "                                  bucket=None):\n",
    "    '''\n",
    "    creates tf.keras.preprocessing.text.tokenizer from a \n",
    "    json config file in current working directory\n",
    "    args:\n",
    "        file_name: str, filename where config json file is located\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file, if an arg\n",
    "            is passed, function will first check if file_name exists in current\n",
    "            directory, and if not, will download an object located at file_name\n",
    "            in the bucket passed into bucket arg\n",
    "    returns:\n",
    "        tokenizer object\n",
    "    '''\n",
    "    if bucket:\n",
    "        if not os.path.isfile(file_name):\n",
    "            download_file(bucket,file_name)\n",
    "    with open(file_name) as file:\n",
    "        open_file = json.load(file)\n",
    "        tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(open_file)\n",
    "    return tokenizer\n",
    "\n",
    "def get_embedding_weights_from_tokenizer_glove(glove_file,\n",
    "                                              tokenizer,\n",
    "                                              embedding_dim,\n",
    "                                              bucket=None,\n",
    "                                              ):\n",
    "    '''\n",
    "    gets the weights to use in an embedding layer from a pretained\n",
    "    model based on the tokenizer used to create sequences that will\n",
    "    be passed into embedding layer\n",
    "    \n",
    "    args:\n",
    "        glove_file: str, path of pretrained model from current directory\n",
    "        tokenizer: tf.keras.preprocessing.text.tokenizer object, tokenizer\n",
    "            that was used to create sequences\n",
    "        embedding_dim: int, output_dim of embedding layer of pre-trained model\n",
    "        bucket, str, default None, name of GCS bucket with an object with the\n",
    "            same file name as glove_file, if an arg\n",
    "            is passed, function will first check if glove_file exists in current\n",
    "            directory, and if not, will download an object located at glove_file\n",
    "            in the bucket passed into bucket arg\n",
    "    returns: \n",
    "        embedding_weights: numpy array, shaped* (vocab_size, embedding_dim)\n",
    "            weights that can be used for embedding layer\n",
    "            *vocab_size = tokenizer.num_words which is the number of words in\n",
    "            the tokenizer vocabulary\n",
    "        \n",
    "    '''\n",
    "    if bucket:\n",
    "        if not os.path.isfile(glove_file):\n",
    "            download_file(bucket, glove_file)\n",
    "    word_values = dict()\n",
    "    file = open(glove_file, encoding='utf-8')\n",
    "    \n",
    "    for line in file:\n",
    "        coeff = line.split()\n",
    "        word = coeff[0]\n",
    "        coefficients = np.asarray(coeff[-embedding_dim:], dtype='float32')\n",
    "        word_values[word] = coefficients\n",
    "    file.close()\n",
    "    vocab_size = tokenizer.num_words\n",
    "    embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx < vocab_size:\n",
    "            word_embedding_values = word_values.get(word)\n",
    "            if word_embedding_values is not None:\n",
    "                embedding_weights[idx] = word_embedding_values\n",
    "    \n",
    "    return embedding_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kx474nk2JHsm"
   },
   "outputs": [],
   "source": [
    "ds = create_ds(tfrecords, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NKb5yx6PJHsr"
   },
   "outputs": [],
   "source": [
    "def create_model(params, embedding_weights=None):\n",
    "    '''\n",
    "    creates model to caption images\n",
    "    '''\n",
    "    vocab_size = params['vocab_size']\n",
    "    txt_input_length = params['text_input_length']\n",
    "    nodes = params['nodes']\n",
    "    embedding_dim = params['embedding_dim']\n",
    "\n",
    "    image_feature_inp = layers.Input((2048,), name='features_input')\n",
    "    \n",
    "    \n",
    "    txt_inp = layers.Input((txt_input_length,), name='text_input')\n",
    "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(txt_inp)\n",
    "\n",
    "    \n",
    "    encoder = layers.Dense(embedding_dim, activation='relu')(embedding)\n",
    "    \n",
    "    w1 = layers.Dense(nodes)(image_feature_inp)\n",
    "    w2 = layers.Dense(nodes)(encoder)\n",
    "    attention = layers.Add()([w1, w2])\n",
    "    attention = layers.Dense(1)(attention)\n",
    "    attention = tf.nn.tanh(attention)\n",
    "    encoder_model = keras.Model([txt_inp, image_feature_inp], [attention, encoder])\n",
    "    \n",
    "    image_context_input = layers.Input((embedding_dim,), name='context_vector')\n",
    "    image_context = tf.expand_dims(image_context_input, 1)\n",
    "    \n",
    "    x = tf.concat([image_context, embedding], axis=1)\n",
    "    x = layers.LSTM(units=nodes, return_sequences=False,)(x)\n",
    "    x = layers.Dense(nodes)(x)\n",
    "    decoder_output = layers.Dense(vocab_size)(x)\n",
    "    decoder_model = keras.Model([image_context_input, txt_inp], [decoder_output])\n",
    "    if embedding_weights is not None:\n",
    "        decoder_model.layers[3].set_weights([embedding_weights])\n",
    "        decoder_model.layers[3].trainable = False\n",
    "        \n",
    "        encoder_model.layers[1].set_weights([embedding_weights])\n",
    "        encoder_model.layers[1].trainable = False\n",
    "\n",
    "    image_input = layers.Input(2048, name='image_input')\n",
    "    text_input = layers.Input(params['text_input_length'], name='text_input')\n",
    "    attention_output, encoder_output = encoder_model((text_input, image_input))\n",
    "    attention_weights = tf.nn.softmax(attention_output, axis=1)\n",
    "    # context_vector = attention_weights * encoder_output\n",
    "    context_vector = tf.multiply(x=attention_weights, y=encoder_output)\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    preds = decoder_model((context_vector, text_input))\n",
    "    final_model = keras.Model([image_input, text_input], preds)\n",
    "    # final_model.compile(optimizer='adam',loss='SparseCategoricalCrossentropy')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-8DlDjMDZf6A"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer_from_filename('coco_tokenizer.json', \n",
    "                                           training_bucket)\n",
    "embedding_weights = get_embedding_weights_from_tokenizer_glove('glove.840B.300d.txt',\n",
    "                                                               tokenizer,\n",
    "                                                               300,\n",
    "                                                               glove_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JkmvmncofYQM"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    '''\n",
    "    taken from https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "    '''\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    loss = tf.reduce_mean(loss_)\n",
    "    #update loss tracker\n",
    "    # loss_tracker.update_state(loss)\n",
    "\n",
    "    return loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-3DhhxMuIsru"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # encoder, decoder = create_model(params, embedding_weights) #embedding_weights\n",
    "    model = create_model(params, embedding_weights)\n",
    "    optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "    model.compile(optimizer='adam', loss=loss_function)\n",
    "    # loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "    # model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "bVGtPCLSJHst",
    "outputId": "d8c515d8-03ea-4fa9-99e2-d170f4bdca57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                patience=15,\n",
    "                                mode='min',\n",
    "                                restore_best_weights=True)\n",
    "for x, y in ds:\n",
    "    history = model.fit(x, y, steps_per_epoch=150, epochs=5000, verbose=0,\n",
    "                        callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "s2Nm6UP7e2jz",
    "outputId": "83e1ed1f-bfd0-4ac1-debf-d3686b529c64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    credentials=None\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "\n",
    "\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file( #file location of GCS private key\n",
    "        '/Users/jeremiahherberg/Downloads/hateful-memes-af65c70c1b79.json')\n",
    "\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "Z5I0r55eZf6J",
    "outputId": "1d0f1108-2b33-4858-9c90-b7b9a9b7053a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "model_num = params['version']\n",
    "model_path = 'image_caption_model_v{}.h5'.format(model_num)\n",
    "model.save(model_path)\n",
    "model_bucket = client.bucket('jh_hateful_memes')\n",
    "blob = model_bucket.blob(model_path)\n",
    "blob.upload_from_filename(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ll0ab_UfVlvi"
   },
   "outputs": [],
   "source": [
    "def get_image_captions(params, images):\n",
    "    '''\n",
    "    creates captions to a group of images\n",
    "    \n",
    "    args:\n",
    "        params: dictionary with at least the following keys:\n",
    "            caption_text_input_length: int, length of captions\n",
    "            tokenizer_start_index: int, value to signal start of caption\n",
    "            tokenizer_end_index: int, value to signal end of caption\n",
    "            \n",
    "        images: tensor, dtype: tf.float32 shaped (None, 299, 299, 3) None is the \n",
    "        number of images, each image should be normalized to have\n",
    "        pixel values of -1 to 1. Images to be captioned\n",
    "\n",
    "            \n",
    "    returns:\n",
    "        captions: tensor, dtype float, shaped \n",
    "        (None, params['caption_text_input_length'])None is the number of \n",
    "        images, image caption sequences\n",
    "    '''\n",
    "    num_images = len(images)\n",
    "    caption_len = params['text_input_length']\n",
    "    caption_end_index = params['tokenizer_end_index']\n",
    "\n",
    "    @tf.function\n",
    "    def get_capt(img, txt):\n",
    "        def caption_step(image_, text_):\n",
    "            '''\n",
    "            evaluate model here\n",
    "            '''\n",
    "            txt_ = tf.expand_dims(text_, axis=0)\n",
    "            pred = model((image_, txt_))\n",
    "\n",
    "\n",
    "            return pred\n",
    "        result = strategy.run(caption_step, args=(img, txt))\n",
    "        return result\n",
    "\n",
    "    \n",
    "    captions = list()\n",
    "    for image in range(num_images):\n",
    "        img = images[image]\n",
    "        img = tf.expand_dims(img, axis=0)\n",
    "        txt_input = np.zeros((caption_len))\n",
    "        result = params['tokenizer_start_index']\n",
    "        for idx in range(caption_len):\n",
    "            txt_input[idx] = result\n",
    "            # with tf.device('/TPU:0'):\n",
    "            #     result = caption_step(img, txt_input)\n",
    "                # result = strategy.run(caption_step, args=(img, txt_input))\n",
    "            result = get_capt(img, txt_input)\n",
    "            result = result.numpy()[0] # result.values[0].numpy()[0]\n",
    "            result = tf.argmax(result, axis=0)\n",
    "            if result == caption_end_index:\n",
    "                break\n",
    "        captions.append(txt_input)\n",
    "    captions = tf.convert_to_tensor(captions)\n",
    "    return captions\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IDjhW_VF9HfO"
   },
   "outputs": [],
   "source": [
    "def plot_metric(metric1, ylabel):\n",
    "    plt.plot(history.history[metric1], label=metric1)\n",
    "    # plt.plot(history.history[metric2], label=metric2)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "fVb6NnhPF6FS",
    "outputId": "c7b4fd87-e0a3-4f48-b5a4-7c49cfe8f3cf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric('loss', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jZc8L4MGe2j8",
    "outputId": "391050af-07ed-4698-e080-9c539bf77a96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6597880125045776"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oYiyR_MIsr8"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "image_caption_model_draft-4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
