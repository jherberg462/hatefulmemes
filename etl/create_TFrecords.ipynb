{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file( #file location of GCS private key\n",
    "    '/Users/jeremiahherberg/Downloads/hateful-memes-af65c70c1b79.json')\n",
    "\n",
    "client = storage.Client(project='hateful-memes', credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_file(file_path):\n",
    "    '''\n",
    "    loads jsonl file and creates a list of dicts\n",
    "    \n",
    "    args:\n",
    "        file_path: str, path of jsonl file to load\n",
    "        \n",
    "    returns: list of dicts in the file located at fle_path\n",
    "    '''\n",
    "    with open(file_path) as file:\n",
    "        json_list = list(file)\n",
    "    list_of_jsons = []\n",
    "    for json_line in json_list:\n",
    "        line = json.loads(json_line)\n",
    "        list_of_jsons.append(line)\n",
    "    \n",
    "    return list_of_jsons\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ds = load_jsonl_file('dev.jsonl')\n",
    "train_ds = load_jsonl_file('train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_feature(int_):\n",
    "    '''\n",
    "    creates a feature that is an int to be used in a TFexample\n",
    "    \n",
    "    args:\n",
    "        int_: int, value to be used as the feature\n",
    "        \n",
    "    returns: feature that can be used in a TFexample\n",
    "    '''\n",
    "    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[int_]))\n",
    "    return feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_feature(text):\n",
    "    '''\n",
    "    creates a feature from a string of text to be used in a TFexample\n",
    "    \n",
    "    args:\n",
    "        text: str, string to be used as the feature\n",
    "    \n",
    "    returns: feature that can be used in a TFexample\n",
    "    '''\n",
    "    text_serialized = tf.io.serialize_tensor(text[0]).numpy() #.tolist()\n",
    "    feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[text_serialized]))\n",
    "\n",
    "#     feature = tf.train.Feature(int64_list=tf.train.Int64List(value=text.flatten() ))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageString_feature(img_string):\n",
    "    '''\n",
    "    creates a feature from a bytestring to be used in a TFexample\n",
    "    \n",
    "    args:\n",
    "        img_string: bytestring, image to be used as the feature\n",
    "        \n",
    "    returns: feature that can be used in a TFexample\n",
    "    \n",
    "    intended to be used to put images into TFrecords, however \n",
    "    this can be used for any bytestring\n",
    "    '''\n",
    "    feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_string]))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imgBytestring_from_filePath(path):\n",
    "    '''\n",
    "    converts an image file into a bytestring\n",
    "    \n",
    "    args:\n",
    "        path: str, file path of image file that will be\n",
    "        converted into bystestring\n",
    "        \n",
    "    returns: bytestring of image file\n",
    "    '''\n",
    "    return open(path, 'rb').read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_lemma(doc, remove_stop=False, language='en_core_web_sm'):\n",
    "    '''\n",
    "    transforms each word in a text to lemma words\n",
    "    \n",
    "    args:\n",
    "        doc: str, text to be transformed\n",
    "        \n",
    "        remove_stop, Bool, default: False, if set to True, stopwords\n",
    "        will be removed\n",
    "        \n",
    "        language: str, default: 'en_core_web_sm', \n",
    "    \n",
    "    returns:\n",
    "        lemma_text: str, original text converted to lemma words\n",
    "    '''\n",
    "    lemma_text = ''\n",
    "    nlp = spacy.load(language)\n",
    "    doc = nlp(doc)\n",
    "    for word in doc:\n",
    "        if remove_stop:\n",
    "            if word.is_stop == False:\n",
    "                lemma_text = '{} {}'.format(lemma_text, word.lemma_)\n",
    "        else:\n",
    "            lemma_text = '{} {}'.format(lemma_text, word.lemma_)\n",
    "    return lemma_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc, language='en_core_web_sm'):\n",
    "    '''\n",
    "    removes stopwords from a text\n",
    "    \n",
    "    args:\n",
    "        doc: str, text to be transformed\n",
    "        \n",
    "        language: str, default: 'en_core_web_sm', \n",
    "    \n",
    "    returns:\n",
    "        no_stops: str, original text with stopwords removed\n",
    "    '''\n",
    "    no_stops = ''\n",
    "    nlp = spacy.load(language)\n",
    "    doc = nlp(doc)\n",
    "    for word in doc:\n",
    "        if word.is_stop == False:\n",
    "            no_stops = '{} {}'.format(no_stops, word)\n",
    "    return no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string, tokenizer, padding):\n",
    "    '''\n",
    "    calls .texts_to_sequences on a tokenizer using string as input\n",
    "    \n",
    "    args:\n",
    "        string: str, text to transform into a sequence\n",
    "        tokenizer: keras.preprocessing.text.Tokenizer object\n",
    "        padding: int, length of output vector. If len of output vector is \n",
    "        less than padding, zeros will be added to beginning, if len is greater\n",
    "        than len of output vector, it will be truncated \n",
    "    \n",
    "    returns: output of tokenizer.texts_to_sequences with string as input\n",
    "    with a len of padding\n",
    "    '''\n",
    "    vector = tokenizer.texts_to_sequences([string])\n",
    "    return sequence.pad_sequences(vector, maxlen=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TFexample(dict_, tokenizer, padding):\n",
    "    '''\n",
    "    creates a TFexample with the following features:\n",
    "        image\n",
    "        label\n",
    "        id\n",
    "        text\n",
    "        text_lemma\n",
    "        text_lemma_no_stopwords\n",
    "        text_no_stopwords\n",
    "    \n",
    "    args:\n",
    "        dict_: dictionary with the following keys:\n",
    "            id: int, id of image\n",
    "            img: str, file path of image\n",
    "            label: int, indicator if meme is hateful or not\n",
    "            text: str, text on meme\n",
    "        tokenizer: keras.preprocessing.text.Tokenizer object that will be used to preprocess text\n",
    "        padding: int, length of each text vector. If text length is less, zeros will be added to \n",
    "        beginning, and if the text length is greater than padding, it will be truncated\n",
    "        \n",
    "    \n",
    "    returns: TFexample with above features\n",
    "    '''\n",
    "    \n",
    "    features = {\n",
    "        'image': imageString_feature(get_imgBytestring_from_filePath(dict_['img'])),\n",
    "        'label': int_feature(dict_['label']),\n",
    "        'id': int_feature(dict_['id']),\n",
    "        'text': text_feature(tokenize(dict_['text'], tokenizer, padding)),\n",
    "        #add stopwords and lemons\n",
    "        'text_lemma' : text_feature(tokenize(transform_to_lemma(dict_['text']), tokenizer, padding)),\n",
    "        'text_lemma_no_stopwords' : text_feature(tokenize(transform_to_lemma(dict_['text'], remove_stop=True),\n",
    "                                                          tokenizer, padding)),\n",
    "        'text_no_stopwords' : text_feature( tokenize(remove_stopwords(dict_['text']), tokenizer,\n",
    "                                                     padding))\n",
    "        \n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_TFrecord_gcs(filepath, client, bucket):\n",
    "    '''\n",
    "    function to upload TFrecord filepath to gcs bucket\n",
    "    \n",
    "    intended to be used for TFrecord files, but can be used for any filetype\n",
    "    \n",
    "    args:\n",
    "        filepath: str, path of file to be uploaded\n",
    "        client: gcs google.storage.Client object\n",
    "        bucket: str, existing gcs bucket to upload file to\n",
    "    \n",
    "    returns:\n",
    "        None \n",
    "    '''\n",
    "    gcs_bucket = client.bucket(bucket)\n",
    "    blob = gcs_bucket.blob(filepath)\n",
    "    blob.upload_from_filename(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TFrecord(meme_list, \n",
    "                    start_idx, end_idx,\n",
    "                    tokenizer, padding,\n",
    "                    tfr_file_num, ttl_tfr_files=10):\n",
    "    '''\n",
    "    creates a TFrecord file\n",
    "    \n",
    "    args:\n",
    "        meme_list: list\n",
    "    \n",
    "    \n",
    "    returns:\n",
    "        TFrecord_filepath, str, file path of newly created tfrecord file\n",
    "    '''\n",
    "    TFrecord_filepath = 'hatefulmemes_{}_of_{}.tfrecord'.format(tfr_file_num,\n",
    "                                                               ttl_tfr_files)\n",
    "    with tf.io.TFRecordWriter(TFrecord_filepath) as writer:\n",
    "        for idx in range(start_idx, end_idx + 1):\n",
    "            TFexample = create_TFexample(meme_list[idx], tokenizer, padding)\n",
    "            writer.write(TFexample.SerializeToString())\n",
    "    \n",
    "    return TFrecord_filepath\n",
    "    ###continue working on documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_idxs(meme_list, num_splits=10):\n",
    "    '''\n",
    "    calculate start and end index's of a list in order to split up a list\n",
    "    evenly\n",
    "    \n",
    "    args:\n",
    "        meme_list: list, list that needs to be split up\n",
    "        num_splits, int, default 10, number of splits the list needs to be \n",
    "        split up into\n",
    "    \n",
    "    returns:\n",
    "        idxs: zip of start and end indexes of meme_list that will evenly split up\n",
    "        meme_list by num_splits\n",
    "    \n",
    "    raises:\n",
    "        ValueError: if length of meme_list is not evenly divisible by num_splits\n",
    "    '''\n",
    "    len_ = len(meme_list)\n",
    "    if len_ % num_splits > 0:\n",
    "        raise ValueError('meme_list must be evenly divisible by num_splits')\n",
    "    \n",
    "    start_idxs = []\n",
    "    end_idxs = []\n",
    "    start_idx = 0\n",
    "    end_idx = len_ / num_splits - 1\n",
    "    for _ in range(num_splits):\n",
    "        start_idxs.append(int(start_idx))\n",
    "        end_idxs.append(int(end_idx))\n",
    "        start_idx += len_ / num_splits\n",
    "        end_idx += len_ / num_splits\n",
    "    \n",
    "    idxs = zip(start_idxs, end_idxs)\n",
    "    return idxs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(input_ds, top_words, preprocess_fn):\n",
    "    '''\n",
    "    creates keras.preprocessing.text.Tokenizer object based on\n",
    "    input dataset, top number of words, and nlp preprocessing\n",
    "    functions\n",
    "    \n",
    "    args:\n",
    "        input_ds: list of dicts, each dict has the following key:\n",
    "            'text': str, text that needs to be tokenized\n",
    "        top_words: int, top number of words to be tokenized\n",
    "        preprocess_fn: function, the text in the input_ds will be\n",
    "        passed into this function to train the tokenizer\n",
    "        (in input_ds text will also not be passed into the preprocess_fn)\n",
    "    \n",
    "    returns:keras.preprocessing.text.Tokenizer object\n",
    "    '''\n",
    "    word_list = [] #list of texts\n",
    "    for item in input_ds:\n",
    "        words = item['text']\n",
    "        word_list.append(words)\n",
    "        preprocessed_words = preprocess_fn(words)\n",
    "        word_list.append(preprocessed_words)\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=top_words)\n",
    "    tokenizer.fit_on_texts(word_list)\n",
    "    return tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ds_path, client, bucket, num_splits=10, top_words=20000, padding=41, preprocess=transform_to_lemma):\n",
    "    '''\n",
    "    creates all TFrecord files\n",
    "    '''\n",
    "    ds = load_jsonl_file(ds_path)\n",
    "    tokenizer = create_tokenizer(ds, top_words, preprocess)###\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    json_file_name = 'tokenizer.json'\n",
    "    with open (json_file_name, 'w') as json_file:\n",
    "        json.dump(tokenizer_json, json_file)\n",
    "    #upload tokenizer json file\n",
    "    upload_TFrecord_gcs(json_file_name, client, bucket)\n",
    "    startEnd_idxs = calc_idxs(ds, num_splits)\n",
    "    file_num = 1\n",
    "    for startIdx, endIdx in startEnd_idxs:\n",
    "        TFrecord_path = create_TFrecord(ds, startIdx, endIdx,\n",
    "                                        tokenizer, padding,\n",
    "                                        file_num, num_splits)\n",
    "        upload_TFrecord_gcs(TFrecord_path, client, bucket)\n",
    "        file_num +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('dev.jsonl', client, 'jh_hateful_memes_dev', padding=41, top_words=30000)#padding - 41 for dev, 58 for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
